{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import service_account_path\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "from src.gdrive_api.auth import build_service\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_revisions('1a0XS1q3IKk1GGWvMoItNinJ1O_RuOOOa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = build_service(service_account_path)\n",
    "revisions = get_all_revisions(service, '1a0XS1q3IKk1GGWvMoItNinJ1O_RuOOOa')\n",
    "revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_all_revisions(service, file_id, fields='id,modifiedTime,keepForever,lastModifyingUser'):\n",
    "    try:\n",
    "        # Get the revisions of the file\n",
    "        revisions = service.revisions().list(fileId=file_id, pageSize=1000, fields=f'revisions({fields})').execute()\n",
    "        return revisions['revisions']\n",
    "    except Exception as e:\n",
    "        print(f\"Error for file: {file_id}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def batch_callback_wrapper(file_id, failed_revision_updates):\n",
    "    def batch_callback(request_id, response, exception):\n",
    "        if exception is not None:\n",
    "            # Handle error\n",
    "            print(f\"An error occurred: {exception}\")\n",
    "            failed_revision_updates[file_id].append({'request_id': request_id, 'response': response, 'exception': exception})\n",
    "            print('Total files with some failed revision updates:', len(failed_revision_updates))\n",
    "        else:\n",
    "            pass\n",
    "            #print(f\"Updated revision: {response.get('id')} keepForever: {response.get('keepForever')}\")\n",
    "    return batch_callback\n",
    "\n",
    "def batch_update_revisions_keepForever(service, file_id, revisions, failed_revision_updates, dry_run=False):\n",
    "    # Sort the revisions by 'modifiedTime' from most recent to oldest\n",
    "    sorted_revisions = sorted(revisions, key=lambda r: r['modifiedTime'], reverse=True)\n",
    "    # Initialize the batch counter and batch request\n",
    "    batch_counter = 0\n",
    "    batch = service.new_batch_http_request(callback=batch_callback_wrapper(file_id, failed_revision_updates))\n",
    "    keep_forever_count_set = 0\n",
    "    total_updated_count = 0\n",
    "\n",
    "    for index, revision in enumerate(sorted_revisions):\n",
    "        keep_forever_status = index < 200\n",
    "        if revision.get('keepForever') != keep_forever_status:\n",
    "            if not dry_run:\n",
    "                batch.add(service.revisions().update(\n",
    "                    fileId=file_id,\n",
    "                    revisionId=revision['id'],\n",
    "                    body={'keepForever': keep_forever_status},\n",
    "                    fields='id, keepForever'\n",
    "                ), request_id=revision['id'])\n",
    "            batch_counter += 1\n",
    "            total_updated_count += 1\n",
    "            if keep_forever_status:\n",
    "                keep_forever_count_set += 1\n",
    "            # Execute the batch request after 50 calls and start a new batch. Limit is 100 but character limit on url is 8k. to be safe using 50\n",
    "            if batch_counter >= 50:\n",
    "                try:\n",
    "                    batch.execute()\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to update revisions for file {file_id}: {e}\")\n",
    "                # Reset the batch counter and start a new batch request\n",
    "                batch_counter = 0\n",
    "                batch = service.new_batch_http_request(callback=batch_callback_wrapper(file_id, failed_revision_updates))\n",
    "\n",
    "    # Execute any remaining calls in the final batch\n",
    "    if batch_counter > 0:\n",
    "        try:\n",
    "            batch.execute()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to update revisions for file {file_id}: {e}\")\n",
    "\n",
    "    return keep_forever_count_set, total_updated_count\n",
    "\n",
    "def group_revisions_by_author_and_time(revisions, max_interval=timedelta(hours=1)):\n",
    "    # Sort revisions by descending modifiedTime\n",
    "    sorted_revisions = sorted(revisions, key=lambda x: x['modifiedTime'], reverse=True)\n",
    "\n",
    "    # Group revisions where the time difference between consecutive items is <= max_interval\n",
    "    # and they are by the same author\n",
    "    grouped_revisions = []\n",
    "    current_group = []\n",
    "\n",
    "    for i, revision in enumerate(sorted_revisions):\n",
    "        if i == 0:\n",
    "            current_group.append(revision)\n",
    "        else:\n",
    "            previous_revision = sorted_revisions[i-1]\n",
    "            previous_revision_time = datetime.fromisoformat(previous_revision['modifiedTime'].rstrip('Z'))\n",
    "            current_revision_time = datetime.fromisoformat(revision['modifiedTime'].rstrip('Z'))\n",
    "            same_author = previous_revision['lastModifyingUser']['emailAddress'] == revision['lastModifyingUser']['emailAddress']\n",
    "            if (previous_revision_time - current_revision_time <= max_interval) and same_author:\n",
    "                current_group.append(revision)\n",
    "            else:\n",
    "                grouped_revisions.append(current_group)\n",
    "                current_group = [revision]\n",
    "\n",
    "    # Add the last group if it's not empty\n",
    "    if current_group:\n",
    "        grouped_revisions.append(current_group)\n",
    "\n",
    "    return grouped_revisions\n",
    "\n",
    "def get_most_recent_revisions_from_groups(grouped_revisions):\n",
    "    most_recent_revisions = [group[0] for group in grouped_revisions]\n",
    "    return most_recent_revisions\n",
    "\n",
    "def update_recent_revisions_to_keep_forever(service_account_path, file_id, dry_run, failed_revision_updates, max_grouping_interval=timedelta(hours=1)):\n",
    "    service = build_service(service_account_path)\n",
    "    try:\n",
    "        revisions = get_all_revisions(service, file_id)\n",
    "        total_revisions_count = len(revisions) if revisions else 0\n",
    "        if revisions:\n",
    "            grouped_revisions = group_revisions_by_author_and_time(revisions, max_interval=max_grouping_interval)\n",
    "\n",
    "            most_recent_revisions_from_groups = get_most_recent_revisions_from_groups(grouped_revisions)\n",
    "            most_recent_revisions_from_groups_count = len(most_recent_revisions_from_groups)\n",
    "            revisions = most_recent_revisions_from_groups\n",
    "        else:\n",
    "            most_recent_revisions_from_groups_count = 0\n",
    "        \n",
    "        result = {'total_revisions_count': total_revisions_count,\n",
    "                  'total_revision_groups_count': most_recent_revisions_from_groups_count,\n",
    "                  'keep_forever_count_set': 0,\n",
    "                  'total_updated_count': 0}\n",
    "        if revisions:\n",
    "            keep_forever_count_set, total_updated_count = batch_update_revisions_keepForever(service, file_id, revisions, failed_revision_updates, dry_run=dry_run)\n",
    "            result['keep_forever_count_set'] = keep_forever_count_set\n",
    "            result['total_updated_count'] = total_updated_count\n",
    "            print(f'Set keepForever={keep_forever_count_set}/{most_recent_revisions_from_groups_count} revisions, updated {total_updated_count} total revisions for file: {file_id}')\n",
    "        else:\n",
    "            print(f\"No revisions found for file {file_id}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to update revisions for {file_id}: {e}\")\n",
    "        return {'total_revisions_count': 0, 'total_revision_groups_count': 0, 'keep_forever_count_set': 0, 'total_updated_count': 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_recent_revisions_to_keep_forever('10P6VHfZQ9FmoW4nrJ41VYVH9NeuCnxws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def extract_id_from_url(url):\n",
    "    \"\"\"\n",
    "    Extract the file ID from a Google Drive URL.\n",
    "    \"\"\"\n",
    "    parts = url.split('/')\n",
    "    if 'drive' in parts:\n",
    "        return parts[4].split('#')[0].split('?')[0]\n",
    "    return None\n",
    "\n",
    "def update_files_in_parallel(service_account_path, file_ids, failed_revision_updates=None, max_workers=10, dry_run=False):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(tqdm(executor.map(lambda file_id: update_recent_revisions_to_keep_forever(service_account_path, file_id, dry_run, failed_revision_updates), file_ids), total=len(file_ids)))\n",
    "    return results\n",
    "\n",
    "def summarize_stats(results):\n",
    "    stats = {\n",
    "        'total_revisions_count': [],\n",
    "        'total_revision_groups_count': [],\n",
    "        'keep_forever_count_set': [],\n",
    "        'total_updated_count': []\n",
    "    }\n",
    "\n",
    "    for result in results:\n",
    "        for key in stats:\n",
    "            stats[key].append(result[key])\n",
    "\n",
    "    stats_summary = {\n",
    "        'sum': {key: sum(values) for key, values in stats.items()},\n",
    "        'max': {key: max(values) for key, values in stats.items()},\n",
    "        'min': {key: min(values) for key, values in stats.items()},\n",
    "        'average': {key: pd.Series(values).mean() for key, values in stats.items()}\n",
    "    }\n",
    "\n",
    "    print(\"Stats Summary:\")\n",
    "    for stat_type, summary in stats_summary.items():\n",
    "        print(f\"{stat_type.capitalize()}:\")\n",
    "        for key, value in summary.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "    return stats_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "grouped_revisions = group_revisions_by_author_and_time(revisions, max_interval=timedelta(hours=1))\n",
    "\n",
    "most_recent_revisions_from_groups = get_most_recent_revisions_from_groups(grouped_revisions)\n",
    "\n",
    "print(grouped_revisions)\n",
    "most_recent_revisions_from_groups\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = ['1a0XS1q3IKk1GGWvMoItNinJ1O_RuOOOa', '1RNw9fAbgXXvovrEfuia-jEtNkd0VOBiH', '10P6VHfZQ9FmoW4nrJ41VYVH9NeuCnxws']\n",
    "failed_revision_updates = defaultdict(list)\n",
    "stats = update_files_in_parallel(service_account_path, file_ids, max_workers=2, dry_run=True, failed_revision_updates=failed_revision_updates)\n",
    "print(summarize_stats(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_delivered_df\n",
    "\n",
    "df = get_delivered_df([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_ids = df['task_link'].apply(extract_id_from_url).to_list()\n",
    "len(file_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats1 = update_files_in_parallel(service_account_path, file_ids, max_workers=10, dry_run=True)\n",
    "print(summarize_stats(stats1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_delivered_df([5])\n",
    "file_ids = df['task_link'].apply(extract_id_from_url).to_list()\n",
    "print(len(file_ids))\n",
    "stats5 = update_files_in_parallel(service_account_path, file_ids, max_workers=10, dry_run=True)\n",
    "print(summarize_stats(stats5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sheets_utils import download_sheet_as_df\n",
    "tracking_sheet_id = '1qBU7Kvuuij2fxbqPxebReKMxWgIBmOIE5Gi4ZuX0j_4'\n",
    "\n",
    "\n",
    "def get_current_batches_df(batch_ids=[1, 2, 3, 4, 5, 6]):\n",
    "    current_work_dfs = []\n",
    "    for batch_id in batch_ids:\n",
    "        print(f\"Downloading batch {batch_id}...\")\n",
    "        df = download_sheet_as_df(\n",
    "            service_account_path, tracking_sheet_id, f\"Conversations_Batch_{batch_id}\"\n",
    "        )\n",
    "        print(f\"Batch {batch_id} downloaded.\")\n",
    "        df = df.assign(batch_id=batch_id)\n",
    "        current_work_dfs.append(df)\n",
    "    current_work_df = pd.concat(current_work_dfs, ignore_index=True)\n",
    "    # Remove duplicate entries based on 'task_link'\n",
    "    current_work_df['file_id'] = current_work_df['task_link'].apply(extract_id_from_url)\n",
    "    current_work_df = current_work_df.drop_duplicates(subset='file_id', keep='last')\n",
    "    print(\"Done.\")\n",
    "    return current_work_df\n",
    "\n",
    "\n",
    "df = get_current_batches_df([1, 2, 3, 4, 5, 6])\n",
    "file_ids = df[df['completion_status'] == 'Done']['file_id'].to_list()\n",
    "print(len(file_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['1sI5fewjKVoEWwO3OVuVzOBvVVER0ZYso',\n",
    "'15AXjZoIhlO9GZl9BTKxSh9JHJkIl_Qh2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_revision_updates = defaultdict(list)\n",
    "stats_all = update_files_in_parallel(service_account_path, file_ids, failed_revision_updates=failed_revision_updates, max_workers=10, dry_run=False)\n",
    "print(summarize_stats(stats_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rerun for', len(failed_revision_updates))\n",
    "second_run_stats_all = update_files_in_parallel(service_account_path, list(failed_revision_updates.keys()), failed_revision_updates=failed_revision_updates, max_workers=10, dry_run=False)\n",
    "print(summarize_stats(second_run_stats_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(failed_revision_updates.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_revision_updates['1XJeEyRu7hsqFakQUtwPBfHzgmpuA8inW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed_revision_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_revision_updates['1RBuZglp5fQFRfmKaAfGRzvzHOPygz-1s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_revision_before_timestamp(file_id, timestamp):\n",
    "    try:\n",
    "        # Get the revisions of the file\n",
    "        revisions = drive_service.revisions().list(fileId=file_id).execute()\n",
    "    except Exception as e:\n",
    "        print(f\"File not found: {file_id}\")\n",
    "        return None\n",
    "\n",
    "    # Convert the timestamp to a datetime object\n",
    "    \n",
    "    timestamp = pd.to_datetime(timestamp).tz_localize(gmt_plus_2h_timezone)\n",
    "\n",
    "    # Initialize the latest revision before the timestamp as None\n",
    "    latest_revision_before_timestamp = None\n",
    "\n",
    "    for revision in revisions['revisions']:\n",
    "        # Convert the modifiedTime of the revision to a datetime object\n",
    "        modified_time = pd.to_datetime(revision['modifiedTime'])\n",
    "        # If the modifiedTime is before the timestamp\n",
    "        if modified_time <= timestamp:\n",
    "            # If this is the first revision or this revision is later than the latest found so far\n",
    "            if latest_revision_before_timestamp is None or modified_time > pd.to_datetime(latest_revision_before_timestamp['modifiedTime']):\n",
    "                # Update the latest revision before the timestamp\n",
    "                latest_revision_before_timestamp = revision\n",
    "\n",
    "    return latest_revision_before_timestamp\n",
    "\n",
    "\n",
    "\n",
    "def get_file_id_from_task_link(task_link):\n",
    "    try:\n",
    "        return task_link.split(\"/\")[-1].split('#')[0]\n",
    "    except Exception as e:\n",
    "        print('ERROR' + '='*60)\n",
    "        print(task_link)\n",
    "        return None\n",
    "\n",
    "## Add a new column to the DataFrame for the file IDs\n",
    "#selected_rows_df['file_id'] = selected_rows_df['Task Link [Google Colab]'].apply(get_file_id_from_task_link)\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "#selected_rows_df['revision'] = selected_rows_df.apply(lambda row: get_revision_before_timestamp(row['file_id'], row['Timestamp']) if row['file_id'] is not None else None, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
