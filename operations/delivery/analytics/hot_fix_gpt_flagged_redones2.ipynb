{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saya/chario/upstream_character_tasks/.venv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "import traceback\n",
    "\n",
    "import nbformat\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from fuzzywuzzy import fuzz\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from openai import OpenAI\n",
    "from utils import get_delivered_df, DATA_DIR, PROJECT_ROOT\n",
    "\n",
    "from src.llm_reviewer.constants import Roles\n",
    "from src.llm_reviewer.llm_api import LLMAPIFactory, make_llm_request\n",
    "\n",
    "\n",
    "def download_file(service_account_file, file_id, revision_id=None):\n",
    "    # Authenticate with the service account\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        service_account_file, scopes=[\"https://www.googleapis.com/auth/drive\"]\n",
    "    )\n",
    "    service = build(\"drive\", \"v3\", credentials=credentials)\n",
    "\n",
    "    # Request to download the file, optionally specifying a revision\n",
    "    if revision_id is not None:\n",
    "        request = service.revisions().get_media(fileId=file_id, revisionId=revision_id)\n",
    "    else:\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "    fh = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "\n",
    "    # Download the file\n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print(\"Download progress: %d%%.\" % int(status.progress() * 100))\n",
    "\n",
    "    # Move the buffer's pointer to the beginning\n",
    "    fh.seek(0)\n",
    "\n",
    "    # Read the notebook content as JSON\n",
    "    payload = json.load(fh)\n",
    "    return payload\n",
    "\n",
    "\n",
    "from src.gdrive_api.folder_upload import upload_folder\n",
    "from utils import service_account_path\n",
    "\n",
    "\n",
    "\n",
    "MAIN_DIR = 'https://drive.google.com/drive/folders/1pEC7hlH3DTMUrkEHeDZduG7AyZf2lSRR'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 59\n",
      "DONE: 53\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from utils import service_account_path\n",
    "from src.sheets_utils import download_sheet_as_df\n",
    "\n",
    "from utils import get_delivered_df\n",
    "\n",
    "batch_5 = get_delivered_df([5])\n",
    "batch_5 = batch_5.assign(jsonl_file_id=batch_5['jsonl_link'].apply(lambda x: x.split('/file/d/')[1].split('/')[0]))\n",
    "\n",
    "sheet_id = '1qBU7Kvuuij2fxbqPxebReKMxWgIBmOIE5Gi4ZuX0j_4'\n",
    "redo_df = download_sheet_as_df(service_account_path, sheet_id, 'gpt_flags_2')\n",
    "print('Total:', len(redo_df))\n",
    "done_df = redo_df[redo_df['status'] == 'Done']\n",
    "done_df = done_df.merge(batch_5[['task_link', 'jsonl_file_id']], on='task_link', how='left')\n",
    "done_df = done_df.assign(file_id=done_df['task_link'].apply(lambda x: x.split('/')[-1]))\n",
    "print('DONE:', len(done_df))\n",
    "#done_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonl_dump import *\n",
    "\n",
    "\n",
    "\n",
    "def download_parse_delivered_into_jsonl(\n",
    "    batch_df, max_workers=20, no_work=False\n",
    "):\n",
    "    delivered_df = batch_df  # Assuming batch_id 4 is required\n",
    "    if not no_work:\n",
    "        parsed_conversations = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            file_ids = [link.split(\"/\")[-1] for link in delivered_df[\"task_link\"]]\n",
    "            parsed_conversations = list(\n",
    "                executor.map(\n",
    "                    download_and_parse_notebook,\n",
    "                    [service_account_path] * len(file_ids),\n",
    "                    file_ids,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for conversation in parsed_conversations:\n",
    "            if conversation is None:\n",
    "                continue\n",
    "            colab_link = conversation[\"colab_link\"]\n",
    "            batch_id = None\n",
    "            for b_id, task_link in zip(\n",
    "                delivered_df[\"batch_id\"], delivered_df[\"task_link\"]\n",
    "            ):\n",
    "                if colab_link.endswith(task_link.split(\"/\")[-1]):\n",
    "                    batch_id = b_id\n",
    "                    break\n",
    "            if batch_id is None:\n",
    "                raise\n",
    "            batch_name = f\"batch_{batch_id}\"\n",
    "            batch_folder = f\"{DATA_DIR}jsonl_conversations/{batch_name}/\"\n",
    "            if not os.path.exists(batch_folder):\n",
    "                os.makedirs(batch_folder)\n",
    "            drive_id = conversation[\"id\"]\n",
    "            with open(f\"{batch_folder}{drive_id}.json\", \"w\") as f:\n",
    "                f.write(json.dumps({\"batch_id\": batch_id, **conversation}))\n",
    "    else:\n",
    "        parsed_conversations = []\n",
    "        for batch_id in batch_ids:\n",
    "            batch_name = f\"batch_{batch_id}\"\n",
    "            batch_folder = f\"{DATA_DIR}jsonl_conversations/{batch_name}/\"\n",
    "            if os.path.exists(batch_folder):\n",
    "                for file_name in os.listdir(batch_folder):\n",
    "                    if file_name.endswith(\".json\"):\n",
    "                        with open(f\"{batch_folder}{file_name}\", \"r\") as f:\n",
    "                            conversation = json.load(f)\n",
    "                            parsed_conversations.append(conversation)\n",
    "            else:\n",
    "                raise Exception(f\"Batch folder for {batch_name} not found\")\n",
    "\n",
    "    return {\"delivered_df\": delivered_df, \"conversations\": parsed_conversations}\n",
    "\n",
    "batch_5['batch_id'] = 66\n",
    "\n",
    "batch_df = batch_5[batch_5['task_link'].isin(done_df['task_link'])]\n",
    "data = download_parse_delivered_into_jsonl(batch_df)\n",
    "print('Downloaded jsonl', len(data['conversations']))\n",
    "\n",
    "import pandas as pd\n",
    "convos = data['conversations']\n",
    "# Create a DataFrame with only 'colab_link' and 'messages' from the conversations list\n",
    "convos_df = pd.DataFrame(convos, columns=['colab_link', 'messages'])\n",
    "merged_df = pd.merge(done_df, convos_df, left_on='task_link', right_on='colab_link')\n",
    "\n",
    "merged_df['jsonl_raw'] = merged_df['jsonl_file_id'].apply(lambda x: download_file(service_account_path, x))\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    row['jsonl_raw']['messages'] = row['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['colab_file_id'] = merged_df['task_link'].apply(lambda x: x.split('drive/')[1])\n",
    "\n",
    "\n",
    "\n",
    "from src.gdrive_api.folder_upload import upload_file\n",
    "from src.gdrive_api.auth import build_service\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    print(f\"Processing {index+1}/{len(merged_df)}\")\n",
    "    json_file_name_only_name = row['colab_file_id'].replace(' ', '') + '.json'\n",
    "    os.makedirs(DATA_DIR + 'FIXED5/', exist_ok=True)\n",
    "    jsonl_filename = DATA_DIR + 'FIXED5/' + json_file_name_only_name\n",
    "    with open(jsonl_filename, 'w') as jsonl_file:\n",
    "        json.dump(row['jsonl_raw'], jsonl_file)\n",
    "    upload_file(build_service(service_account_path), jsonl_filename, '1pEC7hlH3DTMUrkEHeDZduG7AyZf2lSRR', force_replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
