{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_account_file = '../../creds/google__sa.json'\n",
    "\n",
    "tracking_sheet_id = \"1qBU7Kvuuij2fxbqPxebReKMxWgIBmOIE5Gi4ZuX0j_4\"\n",
    "included_sheet_names = [\n",
    "    \"Conversations_Batch_7\",\n",
    "    \"Conversations_Batch_8\",\n",
    "    \"Conversations_Batch_9\",\n",
    "]\n",
    "\n",
    "jupyter_gdrive_folder_ids = [\n",
    "    \"1Z1bdYMe2Qmo_vs-OaKDaYIiV3rIqLJH9\", # V0\n",
    "    \"1sfPFHkXYpKyY41V0pfz3Qw3k4VLy5Hvb\", # V1\n",
    "    \"1jV7WA5zB172DJUp7Z2XzHr62E6U6_NtY\",\n",
    "]\n",
    "\n",
    "delivery_sheet_id = \"1eUif5I8xhHU8fY0X9v8r2JI9hWPh7Dq_9VXpSIHwww4\"\n",
    "delivery_jsonl_gdrive_folder_id = \"1kVHndPCu_WRrgF5rwzUfY_9ZmMG_Jyeh\"\n",
    "destination_folder_url = f\"https://drive.google.com/drive/folders/{delivery_jsonl_gdrive_folder_id}\"\n",
    "DELIVERY_BATCH_NAME = \"Batch 8\"\n",
    "\n",
    "insights_sheet_id = \"1v_O33STdi_h7taPd3MkD0fiqRx7rqr_aAQWGnlOfr_w\"\n",
    "INSIGHTS_VERSION_TAB = \"v1 (Jan 25)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../../')\n",
    "import json\n",
    "import io\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from tqdm import tqdm\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload, MediaIoBaseUpload\n",
    "\n",
    "\n",
    "\n",
    "def crawl_gdrive_folder(folder_id, service_account_file, file_extension=None):\n",
    "    \"\"\"\n",
    "    Crawls a Google Drive folder and returns its contents as a list of items. \n",
    "    Every item can either be a file or a folder. \n",
    "    Folder items are represented as dictionaries with the keys 'id', 'name' and 'children'.\n",
    "    File items are represented as dictionaries with the keys 'id' and 'name'.\n",
    "\n",
    "    Parameters\n",
    "    folder_id: str\n",
    "        The ID of the Google Drive folder to crawl\n",
    "    service_account_file: str\n",
    "        The path to the service account file\n",
    "    file_extension: str\n",
    "        The file extension to filter for. If None, all files are returned.\n",
    "\n",
    "    Returns a list of dictionaries\n",
    "    \"\"\"\n",
    "    # Initialize Google Drive API\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "    credentials = Credentials.from_service_account_file(service_account_file, scopes=SCOPES)\n",
    "    service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "    # Crawl the folder\n",
    "    items = []\n",
    "    page_token = None\n",
    "    while True:\n",
    "        response = service.files().list(\n",
    "            q=f\"'{folder_id}' in parents\",\n",
    "            spaces='drive',\n",
    "            fields='nextPageToken, files(id, name, mimeType)',\n",
    "            pageToken=page_token\n",
    "        ).execute()\n",
    "        for file in response.get('files', []):\n",
    "            if file.get('mimeType') == 'application/vnd.google-apps.folder':\n",
    "                items.append({\n",
    "                    'id': file['id'],\n",
    "                    'name': file['name'],\n",
    "                    'children': crawl_gdrive_folder(file['id'], service_account_file, file_extension)\n",
    "                })\n",
    "            elif file_extension is None or file['name'].endswith(file_extension):\n",
    "                items.append({\n",
    "                    'id': file['id'],\n",
    "                    'name': file['name']\n",
    "                })\n",
    "        page_token = response.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break\n",
    "    return items\n",
    "\n",
    "\n",
    "\n",
    "def download_gdrive_file(file_id, service_account_file):\n",
    "    \"\"\"\n",
    "    Downloads a file from Google Drive, attempting to parse it as a JSON file, returning the parsed contents.\n",
    "\n",
    "    Parameters\n",
    "    file_id: str\n",
    "        The ID of the file to download\n",
    "    service_account_file: str\n",
    "        The path to the service account file\n",
    "\n",
    "    Returns the parsed contents of the file as a dict, or None if the file could not be downloaded\n",
    "    \"\"\"\n",
    "    # Initialize Google Drive API\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "    credentials = Credentials.from_service_account_file(service_account_file, scopes=SCOPES)\n",
    "    service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "    # Download the file\n",
    "    try:\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "        fh = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "        \n",
    "        # Load file contents\n",
    "        fh.seek(0)\n",
    "        return json.load(fh)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing file contents: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def reupload_gdrive_file(file_contents, file_id, service_account_file):\n",
    "    \"\"\"\n",
    "    Re-uploads a dictionary from memory as a JSON file to Google Drive. \n",
    "    If the file already exists, it should present the same id, creating a new version of the file in drive.\n",
    "\n",
    "    Parameters:\n",
    "    - file_contents: dict\n",
    "        The file contents to upload.\n",
    "    - file_id: str\n",
    "        The ID of the file to upload.\n",
    "    - service_account_file: str\n",
    "        The path to the service account file.\n",
    "\n",
    "    Returns True if the file was successfully uploaded, False otherwise.\n",
    "    \"\"\"\n",
    "    # Initialize Google Drive API\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "    credentials = Credentials.from_service_account_file(service_account_file, scopes=SCOPES)\n",
    "    service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "    # Convert dictionary to bytes and prepare for upload\n",
    "    file_stream = io.BytesIO(json.dumps(file_contents).encode())\n",
    "    media = MediaIoBaseUpload(file_stream, mimetype='application/json', resumable=True)\n",
    "\n",
    "    try:\n",
    "        # Perform the update operation\n",
    "        service.files().update(fileId=file_id, media_body=media).execute()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error re-uploading file: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def parallel_execute_with_progress(function, arguments, max_workers=10):\n",
    "    \"\"\"\n",
    "    Executes a function in parallel with multiple arguments displaying a tqdm progress bar.\n",
    "\n",
    "    Parameters\n",
    "    function: function\n",
    "        The function to execute\n",
    "    arguments: list\n",
    "        A list of tuples, where each tuple contains the arguments to pass to the function\n",
    "    max_workers: int\n",
    "        The maximum number of workers to use\n",
    "\n",
    "    Returns a list of results\n",
    "    \"\"\"\n",
    "    # Create a thread pool\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Execute the function for each argument\n",
    "        futures = [executor.submit(function, *args) for args in arguments]\n",
    "        results = []\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(future.result())\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_batches_parent_folder_id = \"1AhR0jAYwgJM1iE_5XSoQvsgcQnKhX4Rc\"\n",
    "pointers = crawl_gdrive_folder(delivery_batches_parent_folder_id, service_account_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: Batch 10, Children: 561\n",
      "Folder: Batch 9, Children: 1305\n",
      "Folder: Batch 8, Children: 559\n",
      "Folder: Batch 7, Children: 492\n",
      "Folder: Batch 6, Children: 482\n",
      "Folder: Redo 1-5 (Jan 25 Feedback), Children: 2995\n",
      "Folder: Batch 5, Children: 1060\n",
      "Folder: Batch 4, Children: 512\n",
      "Folder: Batch 3, Children: 701\n",
      "Folder: Batch 2, Children: 446\n",
      "Folder: Batch 1, Children: 300\n"
     ]
    }
   ],
   "source": [
    "for p in pointers:\n",
    "    if 'children' in p:\n",
    "        print(f\"Folder: {p['name']}, Children: {len(p['children'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'task_link': 'https://colab.research.google.com/drive/1Q43jfkwuqyyG7TzaIooq7IDOy35dV0Pe',\n",
       "  'number_of_turns': 2,\n",
       "  'batch_id': '9',\n",
       "  'domain': {'top_level': 'Web Development', 'sub_level': 'Flask'},\n",
       "  'action': 'Write code in python',\n",
       "  'use_case__summary': 'User seeks assistance for Flask app database connection and secure environment variable management.',\n",
       "  'gpt_estimated_duration': 30},\n",
       " 'messages': [{'role': 'User',\n",
       "   'content': \"I'm building a `Flask` app for internal use and need to connect to our `PostgreSQL` database. Can you help me write a Python code using `SQLAlchemy` to create a connection pool and retrieve the first 10 employee records from the `employees` table? I'm looking for a practical example that's easy to adapt.\",\n",
       "   'type': 'markdown'},\n",
       "  {'role': 'Assistant',\n",
       "   'content': \"Here's a code snippet that accomplishes your request:\",\n",
       "   'type': 'markdown'},\n",
       "  {'role': 'Assistant',\n",
       "   'content': 'import os\\nimport psycopg2\\nfrom sqlalchemy import create_engine, Pool\\nfrom sqlalchemy.orm import sessionmaker\\n\\n# Database credentials (You have to store these credentials securely in environment variables in your App)\\nDB_HOST = os.environ.get(\"DB_HOST\")\\nDB_NAME = os.environ.get(\"DB_NAME\")\\nDB_USER = os.environ.get(\"DB_USER\")\\nDB_PASSWORD = os.environ.get(\"DB_PASSWORD\")\\n\\n# Connect string\\nconnection_string = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\"\\n\\ntry:\\n    # Create engine with connection pool\\n    # Adjust the pool_size based on your expected workload and hardware resources.\\n    # The pool_recycle option ensures old connections are refreshed periodically, preventing stale connections.\\n    engine = create_engine(\\n        connection_string, poolclass=Pool, pool_size=5, pool_recycle=3600\\n    )\\n\\n    # Create a session factory\\n    Session = sessionmaker(autocommit=False, autoflush=False, bind=engine)\\n\\n    # Get a session from the factory\\n    session = Session()\\n\\n    try:\\n        # Query for the first 10 employees\\n        employees = session.query(\"employees\").limit(10).all()\\n\\n        # Process employee data (replace the print logic with what you want to do with the first 10 employees)\\n        for employee in employees:\\n            print(f\"Employee ID: {employee.id}, Name: {employee.name}\")\\n\\n    # You can add additional except blocks for specific exceptions like psycopg2.OperationalError for database connectivity issues.\\n    except Exception as e:\\n        # Error handling for database operations within the \\'try\\' block\\n        print(f\"An error occurred during database operations: {str(e)}\")\\n        # Log the error or perform other error handling actions as needed\\n\\n    finally:\\n        # Close the session to release resources\\n        session.close()\\n\\nexcept Exception as e:\\n    # Error handling for connection or setup-related issues\\n    print(f\"An error occurred during setup or connection: {str(e)}\")\\n    # Log the error or perform other error handling actions as needed',\n",
       "   'type': 'code'},\n",
       "  {'role': 'Assistant',\n",
       "   'content': \"* This code uses environment variables for sensitive database credentials, improving security.\\n* A connection pool is created with Pool to handle multiple connections efficiently.\\n* Error handling is included using a `try...except...finally` block to ensure the session is closed even if an exception occurs.\\n\\nIf you have any more queries or any part in the code that is still not clear don't hesitate to ask.\",\n",
       "   'type': 'markdown'},\n",
       "  {'role': 'User',\n",
       "   'content': \"It's been really helpful. Now, I'm trying to figure out how to securely store and access environment variables in my `Flask` app. Is there a recommended approach you could suggest?\",\n",
       "   'type': 'markdown'},\n",
       "  {'role': 'Assistant',\n",
       "   'content': 'Securing environment variables is crucial for your app\\'s security. Here are two common approaches you can consider:\\n\\n**Using a `.env` file** :\\n* Create a file named `.env` in your project\\'s root directory.\\nAdd key-value pairs where the key is the environment variable name and the value is its actual value.\\n```\\n# .env file example\\nDB_HOST=localhost\\nDB_NAME=mydatabase\\nDB_USER=username\\nDB_PASSWORD=password\\n```\\n\\n  Install the `python-dotenv` package: ```pip install python-dotenv```\\nIn your `Flask` app, load the environment variables before accessing them:\\n\\n  ```python\\n  from dotenv import load_dotenv\\n\\n  load_dotenv()\\n\\n  db_host = os.environ.get(\"DB_HOST\")\\n  # ... access other variables\\n  ```\\n\\n**Using Flask config**\\n\\n* Define environment variables in your `Flask` config:\\n\\n  ```python\\n  app.config[\\'DB_HOST\\'] = \\'localhost\\'\\n  app.config[\\'DB_NAME\\'] = \\'mydatabase\\'\\n  # ... add other variables\\n  ```\\n  Access them using `app.config`:\\n  ```python\\n  db_host = app.config[\\'DB_HOST\\']\\n  # ... access other variables\\n  ```\\n\\n\\nBoth approaches have pros and cons:\\n\\n`.env`: Easier to manage for smaller projects, but the file isn\\'t version-controlled.\\n\\n\\n`Flask config`: Securely stored, but managing multiple environments might be complex.',\n",
       "   'type': 'markdown'}],\n",
       " 'id': '1Q43jfkwuqyyG7TzaIooq7IDOy35dV0Pe'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_contents = download_gdrive_file(pointers[0]['children'][1]['id'], service_account_file)\n",
    "test_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reupload_gdrive_file(test_contents, pointers[0]['children'][0]['id'], service_account_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_execute_with_progress(reupload_gdrive_file, [(test_contents, pointers[0]['children'][0]['id'], service_account_file)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.program import OpenAIPydanticProgram\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import concurrent.futures\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "class HierarchicalCategory(BaseModel):\n",
    "    \"\"\"Data model for hierarchical category classification.\"\"\"\n",
    "    top_level: str\n",
    "    sub_level: str\n",
    "\n",
    "def classify_conversation_by_domain(conversation: List[dict]) -> HierarchicalCategory:\n",
    "    DOMAIN_CATEGORIES = \"\"\"\n",
    "        - Python basics & scripting\n",
    "        - Problem Solving\n",
    "        - Interview Prep\n",
    "        - Web Development\n",
    "        - Testing\n",
    "        - Cloud Computing / Frameworks\n",
    "        - Data Analysis\n",
    "        - Machine Learning\n",
    "        - Other languages\n",
    "        - Other\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_template_str = \"\"\"\n",
    "    Categorize the theme of user requests in the following conversation by domain into one of the following top-level categories, then sub categories that you think is descriptive & appropriate:\n",
    "    {categories}\n",
    "\n",
    "    Conversation:\n",
    "    {conversation}\n",
    "    \"\"\"\n",
    "\n",
    "    program = OpenAIPydanticProgram.from_defaults(\n",
    "        llm=OpenAI(api_key=api_key, model=\"gpt-4-1106-preview\", temperature=0),\n",
    "        output_cls=HierarchicalCategory,\n",
    "        prompt_template_str=prompt_template_str,\n",
    "        verbose=False,\n",
    "    )\n",
    "    output = program(\n",
    "        categories=DOMAIN_CATEGORIES,\n",
    "        conversation=conversation[\"messages\"],\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "def classify_conversation_by_action(\n",
    "    conversation: List[dict]\n",
    ") -> HierarchicalCategory:\n",
    "    ACTION_CATEGORIES = \"\"\"\n",
    "    - Write code in python\n",
    "    - Explain code\n",
    "    - Fix / refactor / optimize code\n",
    "    - Debug error trace\n",
    "    - Write unit tests\n",
    "    - Write CI/CD code\n",
    "    - Do a code review\n",
    "    - Write / modify / fix beam code\n",
    "    - Write / modify / fix spark code\n",
    "    - Write end to end ML training code\n",
    "    - Help me take an interview\n",
    "    - Answer ML research questions\n",
    "    - Answer infra questions\n",
    "    - Write / modify / fix SQL code\n",
    "    - Write / modify / fix JavaScript code\n",
    "    - Scrape a website\n",
    "    \"\"\"\n",
    "    prompt_template_str = \"\"\"\n",
    "    Categorize the initial user request in the following conversation by requested action into one of the following top-level categories. Sub-level should be empty string always. In case there's no natural fit, use \"Other\" as the top-level category.\n",
    "    \n",
    "    Categories:\n",
    "    {categories}\n",
    "\n",
    "    Conversation:\n",
    "    {conversation}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    program = OpenAIPydanticProgram.from_defaults(\n",
    "        llm=OpenAI(api_key=api_key, model=\"gpt-4-1106-preview\", temperature=0),\n",
    "        output_cls=HierarchicalCategory,\n",
    "        prompt_template_str=prompt_template_str,\n",
    "        verbose=False,\n",
    "    )\n",
    "    output = program(\n",
    "        categories=ACTION_CATEGORIES,\n",
    "        conversation=conversation[\"messages\"],\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "class SummaryResult(BaseModel):\n",
    "    \"\"\"Data model for the summary result.\"\"\"\n",
    "    summary: str = Field(\n",
    "        description=\"A short summary containing 1 sentence, 15 words max, focused on the specific theme. [super concise language]\"\n",
    "    )\n",
    "\n",
    "def exec_summary(conversation: List[List[dict]]):\n",
    "    prompt_template_str = \"\"\"\n",
    "    Given the following conversation, please, generate an executive summary of the conversation.\n",
    "\n",
    "    User Use Case, why user uses the Assistant in this conversation, in general terms, **for what** the User is using it. Not from a technical perspective, but from a daily life situation perspective. \n",
    "    Example: work, homework, exam, studying, inteview, debugging, etc...\n",
    "\n",
    "    It should also contain a little bit of the context of the conversation, and the main goal of the conversation.\n",
    "\n",
    "    Conversation:\n",
    "    {conversation}\n",
    "    \"\"\"\n",
    "    program = OpenAIPydanticProgram.from_defaults(\n",
    "        llm=OpenAI(api_key=api_key, model=\"gpt-4-1106-preview\", temperature=0),\n",
    "        output_cls=SummaryResult,\n",
    "        prompt_template_str=prompt_template_str,\n",
    "        verbose=False,\n",
    "    )\n",
    "    output = program(\n",
    "        conversation=conversation[\"messages\"]\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "class GPTEstimationResult(BaseModel):\n",
    "    \"\"\"Data model for the GPT estimation result.\"\"\"\n",
    "    estimated_duration: int = Field(\n",
    "        description=\"The estimated duration of demonstrating the conversation in minutes.\"\n",
    "    )\n",
    "\n",
    "def gpt_estimated_duration(conversation: dict) -> int:\n",
    "    prompt_template_str = \"\"\"\n",
    "    Given the following conversation which has been demonstrated by a median skilled technical human playing both User and Assistant... He also is responsible for making sure the assistant responses are flawless...\n",
    "    Estimate how many minutes it would take to Design, Research for, Write & Verify the code in this.\n",
    "    You can consider the following factors to estimate (Conversation Length, Topic/Code Complexity).\n",
    "\n",
    "    If you get this right, you will save my life.\n",
    "\n",
    "    Conversation:\n",
    "    {conversation}\n",
    "    \"\"\"\n",
    "    program = OpenAIPydanticProgram.from_defaults(\n",
    "        llm=OpenAI(api_key=api_key, model=\"gpt-4-1106-preview\", temperature=0),\n",
    "        output_cls=GPTEstimationResult,\n",
    "        prompt_template_str=prompt_template_str,\n",
    "        verbose=False,\n",
    "    )\n",
    "    output = program(\n",
    "        conversation=conversation[\"messages\"]\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "def process_conversation__metadata_extraction(conversation, task_link):\n",
    "    domain = classify_conversation_by_domain(conversation)\n",
    "    action = classify_conversation_by_action(conversation)\n",
    "    summary = exec_summary(conversation)\n",
    "    estimate_duration = gpt_estimated_duration(conversation)\n",
    "    if \"metadata\" in conversation:\n",
    "        conversation[\"metadata\"].update({\n",
    "            \"domain\": domain.model_dump(), \n",
    "            \"action\": action.model_dump()[\"top_level\"], \n",
    "            \"use_case_summary\": summary.model_dump()[\"summary\"],\n",
    "            \"gpt_estimated_duration\": estimate_duration.model_dump()[\"estimated_duration\"],\n",
    "            \"task_link\": task_link\n",
    "        })\n",
    "    else:\n",
    "        conversation[\"metadata\"] = {\n",
    "            \"domain\": domain.model_dump(), \n",
    "            \"action\": action.model_dump()[\"top_level\"], \n",
    "            \"use_case_summary\": summary.model_dump()[\"summary\"],\n",
    "            \"gpt_estimated_duration\": estimate_duration.model_dump()[\"estimated_duration\"],\n",
    "            \"task_link\": task_link\n",
    "        }\n",
    "    return conversation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.program import OpenAIPydanticProgram\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "def transform__concatenate_back_to_back_messages_from_same_role(messages):\n",
    "    \"\"\"\n",
    "    Merge back-to-back user & ai messages into a single message, with the content of the messages concatenated together.\n",
    "    \"\"\"\n",
    "    if len(messages) == 0:\n",
    "        return []\n",
    "\n",
    "    concatenated_messages = []\n",
    "    current_concatenation = \"\"\n",
    "    current_role = messages[0]['role']\n",
    "    for message in messages:\n",
    "        if message.get('role') == current_role:\n",
    "            current_concatenation += message.get('content').strip() + \"\\n\\n\"\n",
    "        else:\n",
    "            concatenated_messages.append({\n",
    "                'role': current_role,\n",
    "                'content': current_concatenation.strip()\n",
    "            })\n",
    "            current_concatenation = message.get('content') + \"\\n\\n\"\n",
    "            current_role = message.get('role')\n",
    "\n",
    "    # Add the last concatenation\n",
    "    concatenated_messages.append({\n",
    "        'role': current_role,\n",
    "        'content': current_concatenation.rstrip()\n",
    "    })\n",
    "    return concatenated_messages\n",
    "\n",
    "\n",
    "\n",
    "def transform__code_blocks_to_syntax_highlighted_md(messages): \n",
    "    \"\"\"\n",
    "    Identify the language of each code block and transform it into markdown syntax highlighting.\n",
    "    \"\"\"\n",
    "    transformed_messages = []\n",
    "\n",
    "    # Isolate Code blocks\n",
    "    for message in messages:\n",
    "\n",
    "        if message.get(\"type\") != \"code\":\n",
    "            transformed_messages.append(message)\n",
    "            continue\n",
    "\n",
    "        message_copy = message.copy()\n",
    "\n",
    "        # Identify the language of each code block\n",
    "        class Language(BaseModel):\n",
    "            \"\"\"Data model for identifying the language of a code block for use in markdown syntax highlighting.\"\"\"\n",
    "            language: str\n",
    "\n",
    "        prompt_template_str = \"\"\"\n",
    "        Given the following conversation, Identify the language of each code block.\n",
    "        The output should be compatible with markdown syntax highlighting for triple backtick code blocks.\n",
    "\n",
    "        Contents:\n",
    "        {code}\n",
    "        \"\"\"\n",
    "        program = OpenAIPydanticProgram.from_defaults(\n",
    "            llm=OpenAI(api_key=api_key, model=\"gpt-4-1106-preview\", temperature=0),\n",
    "            output_cls=Language,\n",
    "            prompt_template_str=prompt_template_str,\n",
    "            verbose=False,\n",
    "        )\n",
    "        output = program(\n",
    "            code=message_copy.get(\"content\").strip()\n",
    "        )\n",
    "        language = output.language\n",
    "\n",
    "        # Strip leading and trailing spaces/newlines from the code block\n",
    "        message_copy[\"content\"] = f\"```{language}\\n{message_copy.get('content').strip()}\\n```\"\n",
    "        message_copy[\"type\"] = \"markdown\"\n",
    "\n",
    "        # Wrap in triple backticks and append to transformed messages\n",
    "        transformed_messages.append(message_copy)\n",
    "\n",
    "    return transformed_messages\n",
    "\n",
    "\n",
    "def is_metadata_v1(file_contents):\n",
    "    \"\"\"\n",
    "    Verify the schema.\n",
    "\n",
    "    Includes metadata dict, which includes these keys\n",
    "    - domain\n",
    "    - action\n",
    "    - use_case_summary\n",
    "    - gpt_estimated_duration\n",
    "    - task_link\n",
    "    \"\"\"\n",
    "    if file_contents is None:\n",
    "        return False\n",
    "    \n",
    "    if \"metadata\" in file_contents:\n",
    "        if \"domain\" in file_contents[\"metadata\"] and \\\n",
    "            \"action\" in file_contents[\"metadata\"] and \\\n",
    "            \"use_case_summary\" in file_contents[\"metadata\"] and \\\n",
    "            \"gpt_estimated_duration\" in file_contents[\"metadata\"] and \\\n",
    "            \"task_link\" in file_contents[\"metadata\"]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def transform_pipeline(file_pointer, update_metadata=False):\n",
    "    try:\n",
    "        # Download the file\n",
    "        file_contents = download_gdrive_file(file_pointer['id'], service_account_file)\n",
    "\n",
    "        # Transform the file\n",
    "        transformed_messages = transform__code_blocks_to_syntax_highlighted_md(file_contents['messages'])\n",
    "        file_contents[\"messages\"] = transform__concatenate_back_to_back_messages_from_same_role(transformed_messages)\n",
    "\n",
    "        # Update the metadata if not v1 schema\n",
    "        if not is_metadata_v1(file_contents) and update_metadata:\n",
    "            colab_id = file_pointer['name'].split(\".\")[0]\n",
    "            colab_link = f\"https://colab.research.google.com/drive/{colab_id}\"\n",
    "            file_contents = process_conversation__metadata_extraction(file_contents, colab_link)\n",
    "\n",
    "        # Re-upload the file\n",
    "        reupload_gdrive_file(file_contents, file_pointer['id'], service_account_file)\n",
    "\n",
    "        return file_contents, None\n",
    "    except Exception as e:\n",
    "        return False, e\n",
    "    \n",
    "\n",
    "def transform__fix_task_link(file_pointer):\n",
    "    try:\n",
    "        # Download the file\n",
    "        file_contents = download_gdrive_file(file_pointer['id'], service_account_file)\n",
    "\n",
    "        colab_id = file_pointer['name'].split(\".\")[0]\n",
    "        colab_link = f\"https://colab.research.google.com/drive/{colab_id}\"\n",
    "        file_contents[\"metadata\"][\"task_link\"] = colab_link\n",
    "\n",
    "        # Re-upload the file\n",
    "        reupload_gdrive_file(file_contents, file_pointer['id'], service_account_file)\n",
    "\n",
    "        return file_contents, None\n",
    "    except Exception as e:\n",
    "        return False, e\n",
    "\n",
    "\n",
    "def transform__fix_use_case_summary_key(file_pointer):\n",
    "    try:\n",
    "        # Download the file\n",
    "        file_contents = download_gdrive_file(file_pointer['id'], service_account_file)\n",
    "\n",
    "        # Update the metadata if not v1 schema\n",
    "        file_contents[\"metadata\"][\"use_case__summary\"] = file_contents[\"metadata\"].pop(\"use_case_summary\")\n",
    "\n",
    "        # Re-upload the file\n",
    "        reupload_gdrive_file(file_contents, file_pointer['id'], service_account_file)\n",
    "\n",
    "        return file_contents, None\n",
    "    except Exception as e:\n",
    "        return False, e\n",
    "\n",
    "\n",
    "# Flatten the pointers recursively\n",
    "def flatten_pointers(pointers, folders=None):\n",
    "    flat_pointers = []\n",
    "    for pointer in pointers:\n",
    "        if folders is None or pointer['name'] in folders:\n",
    "            if 'children' in pointer:\n",
    "                flat_pointers.extend(flatten_pointers(pointer['children']))\n",
    "            else:\n",
    "                flat_pointers.append(pointer)\n",
    "    return flat_pointers\n",
    "\n",
    "flat_pointers = flatten_pointers(pointers, folders=[\"Batch 10\"])\n",
    "len(flat_pointers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 561/561 [03:41<00:00,  2.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Execute the pipeline in parallel on all pointers, Use chunks and Save/Load checkpoints\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "\n",
    "def execute_pipeline_on_pointers(pointers, chunk_size=1500, checkpoint_filename=None):\n",
    "    # Load checkpoint if it exists\n",
    "    if checkpoint_filename is not None and os.path.exists(checkpoint_filename):\n",
    "        checkpoint = load_checkpoint(checkpoint_filename)\n",
    "    else:\n",
    "        checkpoint = {\n",
    "            \"completed\": [],\n",
    "            \"failed\": []\n",
    "        }\n",
    "        \n",
    "    # Execute the pipeline in parallel\n",
    "    for i in range(0, len(pointers), chunk_size):\n",
    "        chunk = pointers[i:i + chunk_size]\n",
    "        results = parallel_execute_with_progress(transform_pipeline, [(pointer,) for pointer in chunk], max_workers=15)\n",
    "        for result, error in results:\n",
    "            if error is not None:\n",
    "                checkpoint[\"failed\"].append(result)\n",
    "            else:\n",
    "                checkpoint[\"completed\"].append(result)\n",
    "\n",
    "        # Save checkpoint\n",
    "        if checkpoint_filename is not None:\n",
    "            save_checkpoint(checkpoint, checkpoint_filename)\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "checkpoint_filename = \"checkpoint.pkl\"\n",
    "checkpoint = execute_pipeline_on_pointers(flat_pointers, checkpoint_filename=checkpoint_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'metadata': {'task_link': 'https://colab.research.google.com/drive/1fpx6m5PT',\n",
       "   'number_of_turns': 3,\n",
       "   'batch_id': '7',\n",
       "   'domain': {'top_level': 'Python basics & scripting',\n",
       "    'sub_level': 'Code maintainability and readability'},\n",
       "   'action': 'Explain code',\n",
       "   'use_case__summary': 'User seeks advice on commenting Python code for a digital library system to enhance maintainability and readability.',\n",
       "   'gpt_estimated_duration': 10,\n",
       "   'use_case_summary': 'User seeks advice on commenting Python code for a digital library system to enhance maintainability and readability.'},\n",
       "  'messages': [{'role': 'User',\n",
       "    'content': \"I've been working on a Python script for a digital library management system. It handles book inventory and user interactions, but I'm concerned about the long-term maintenance and readability of the code. Can you suggest how to effectively add comments to my code?\"},\n",
       "   {'role': 'Assistant',\n",
       "    'content': 'Absolutely! Effective commenting can significantly improve code maintainability and readability. Here are some strategies for commenting your library management system code:\\n\\n\\n*   ***Use descriptive comments for complex logic:***\\n\\nFor any part of your code that handles complex operations, such as sorting books or searching for user data, use comments to describe what the code is doing and why.\\n\\n\\n```\\n# Sort books by title; this helps in faster retrieval during search operations\\nbooks.sort(key=lambda book: book.title)\\n\\n```\\n\\n\\n*   ***Include docstrings*** at the beginning of each function or class to explain the purpose, parameters, and return type. This is crucial for anyone who will use or maintain your code in the future.\\n\\n\\n\\n```\\ndef check_out_book(user_id, book_id):\\n    \"\"\"\\n    Process a book checkout for a user.\\n    :param user_id: ID of the user checking out the book\\n    :param book_id: ID of the book to be checked out\\n    :return: None\\n    \"\"\"\\n\\n```\\n\\n\\n\\n\\n*   ***Mark sections of the script*** for easier navigation. This can help maintainers quickly find parts of the code responsible for specific features like inventory control, user management, etc.\\n\\n\\n\\n```\\n# =====================\\n# Inventory Management\\n# =====================\\n\\n```\\n\\n\\n\\n*   ***Explain non-obvious choices:*** If you\\'ve made a specific decision in your code for performance or design reasons, document it. This helps prevent future developers from changing it without understanding the reasoning.\\n\\n\\n\\n```\\n# Using a dictionary for faster lookups of books by ID, assuming large datasets\\nbook_inventory = {book.id: book for book in book_list}\\n\\n```\\n\\n\\n*   ***Update comments when updating code:*** Ensure that comments are updated along with the code changes to prevent discrepancies that can confuse maintainers.'},\n",
       "   {'role': 'User',\n",
       "    'content': 'That makes sense. How detailed should my docstrings be for the functions managing user interactions?'},\n",
       "   {'role': 'Assistant',\n",
       "    'content': 'Aim for clarity and brevity. For each function managing user interactions, your docstring should include:\\n\\n\\n\\n*   A brief description of the function\\'s purpose.\\n\\n*   Parameters and their expected data types.\\n\\n*   What the function returns or its side effects (e.g., updating a database, sending a notification).\\n\\n*   Any exceptions that might be raised and under what conditions.\\n\\nHere\\'s an example:\\n\\n```python\\ndef add_user(username, email):\\n    \"\"\"\\n    Adds a new user to the library system.\\n\\n    :param username: str, unique username for the user\\n    :param email: str, email address of the user\\n    :return: user_id (int) of the newly added user\\n    :raises ValueError: If the username is already taken or email format is invalid\\n    \"\"\"\\n```'},\n",
       "   {'role': 'User',\n",
       "    'content': 'Great, this will definitely help. Is there a standard for how often I should comment within functions ?'},\n",
       "   {'role': 'Assistant',\n",
       "    'content': 'As a rule of thumb, comment on anything that isn\\'t immediately clear from the code itself. However, avoid over-commenting; your code should be as readable as possible on its own. Use comments to clarify the \"why\" behind a code block, especially if the logic isn\\'t straightforward. For standard operations, such as opening a file or simple arithmetic operations, comments aren\\'t usually necessary unless you\\'re doing something unconventional.\\n\\nRemember, the goal of comments is to make your code easier to understand for someone who might be seeing it for the first time. If a piece of code took you a while to figure out, it\\'s a good candidate for a comment.'}],\n",
       "  'id': '1fpx6m5PT_j6-PsjZgmjMNvXucj9EOomw'},\n",
       " None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pointer = {'id': '1iD6sjEucJO09TBzUjBXoQX0FpbI1wuvL',\n",
    "   'name': '1fpx6m5PT_j6-PsjZgmjMNvXucj9EOomw.json'}\n",
    "transform_pipeline(test_pointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder_names = [\n",
    "    \"Batch 7\",\n",
    "    \"Batch 6\",\n",
    "    \"Redo 1-5 (Jan 25 Feedback)\"\n",
    "]\n",
    "\n",
    "# Download the files\n",
    "def flatten_pointers(pointers, folders=None):\n",
    "    flat_pointers = []\n",
    "    for pointer in pointers:\n",
    "        if folders is None or pointer['name'] in folders:\n",
    "            if 'children' in pointer:\n",
    "                flat_pointers.extend(flatten_pointers(pointer['children']))\n",
    "            else:\n",
    "                flat_pointers.append(pointer)\n",
    "    return flat_pointers\n",
    "\n",
    "# flat_filtered_pointers = flatten_pointers(pointers, current_folder_names)\n",
    "# print(len(flat_filtered_pointers))\n",
    "# results = parallel_execute_with_progress(download_gdrive_file, [(pointer['id'], service_account_file) for pointer in flat_filtered_pointers], max_workers=50)\n",
    "\n",
    "\n",
    "# Download the delivery sheet\n",
    "# from src.sheets_utils import download_sheet_as_df\n",
    "# batch_to_links = {\n",
    "#     \"Batch 1\": set(download_sheet_as_df(service_account_file, delivery_sheet_id, \"Batch 1\")[\"task_link\"].to_list()),\n",
    "#     \"Batch 2\": set(download_sheet_as_df(service_account_file, delivery_sheet_id, \"Batch 2\")[\"task_link\"].to_list()),\n",
    "#     \"Batch 3\": set(download_sheet_as_df(service_account_file, delivery_sheet_id, \"Batch 3\")[\"task_link\"].to_list()),\n",
    "#     \"Batch 4\": set(download_sheet_as_df(service_account_file, delivery_sheet_id, \"Batch 4\")[\"task_link\"].to_list()),\n",
    "#     \"Batch 5\": set(download_sheet_as_df(service_account_file, delivery_sheet_id, \"Batch 5\")[\"task_link\"].to_list()),\n",
    "#     \"Batch 6\": set(download_sheet_as_df(service_account_file, delivery_sheet_id, \"Batch 6\")[\"task_link\"].to_list()),\n",
    "#     \"Batch 7\": set(download_sheet_as_df(service_account_file, delivery_sheet_id, \"Batch 7\")[\"task_link\"].to_list()),\n",
    "# }\n",
    "\n",
    "\n",
    "# Parse their metadata into a df corresponding to the links\n",
    "import pandas as pd\n",
    "insights_records = []\n",
    "for r in results:\n",
    "    if r is None:\n",
    "        continue\n",
    "\n",
    "    batch_id = None\n",
    "    for batch, links in batch_to_links.items():\n",
    "        if r[\"metadata\"].get(\"task_link\", None) in links:\n",
    "            batch_id = batch.split()[1]\n",
    "            break\n",
    "\n",
    "    record = {\n",
    "        \"task_link\": r[\"metadata\"].get(\"task_link\", None),\n",
    "        \"batch_id\": batch_id,\n",
    "        \"number_of_turns\": r[\"metadata\"].get(\"number_of_turns\", None),\n",
    "        \"gpt_estimated_duration\": r[\"metadata\"].get(\"gpt_estimated_duration\", None),\n",
    "        \"action\": r[\"metadata\"].get(\"action\", None),\n",
    "        \"domain__top_level\": r[\"metadata\"].get(\"domain\", {}).get(\"top_level\", None),\n",
    "        \"domain__sub_level\": r[\"metadata\"].get(\"domain\", {}).get(\"sub_level\", None),\n",
    "        \"use_case__summary\": r[\"metadata\"].get(\"use_case_summary\", None)\n",
    "    }\n",
    "    insights_records.append(record)\n",
    "insights_df = pd.DataFrame(insights_records)\n",
    "insights_df = insights_df.fillna(\"\")\n",
    "\n",
    "# Upload the df to the insights sheet\n",
    "# from src.sheets_utils import upload_df_to_sheet, GoogleSheetsService\n",
    "\n",
    "# sheets_client = GoogleSheetsService(service_account_file, ['https://www.googleapis.com/auth/spreadsheets'])\n",
    "# values = [insights_df.columns.tolist()] + insights_df.values.tolist()\n",
    "# sheets_client.update_or_append_data_to_sheet(insights_sheet_id, INSIGHTS_VERSION_TAB, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'task_link': 'https://colab.research.google.com/drive/1ztZJScju_m-9MYF2v1fze2TVgjPadgNF',\n",
       "  'number_of_turns': 4,\n",
       "  'batch_id': '7',\n",
       "  'domain': {'top_level': 'Machine Learning',\n",
       "   'sub_level': 'Neural Network Design'},\n",
       "  'action': 'Write end to end ML training code',\n",
       "  'use_case__summary': 'The user is refining a fitness recommendation system, seeking to enhance it with historical workout data and feedback loops for personalized plans, and exploring strategies for scaling and integrating diverse datasets.',\n",
       "  'gpt_estimated_duration': 60,\n",
       "  'use_case_summary': 'The User seeks to enhance a fitness recommendation system with ML, focusing on scalability and integrating diverse data for holistic advice.'},\n",
       " 'messages': [{'role': 'User',\n",
       "   'content': \"As we refine our ML-driven fitness recommendation system, we've focused on tailoring workout plans according to individual progress and feedback. I'm considering incorporating a user's historical workout data into the model. Could you demonstrate how to structure a neural network in TensorFlow that dynamically updates the user's workout plan based on their fitness data and feedback loop?\"},\n",
       "  {'role': 'Assistant',\n",
       "   'content': \"Certainly! To design a dynamic neural network that adapts workout recommendations based on user feedback, you might want to consider a model that processes sequential data, such as an LSTM (Long Short-Term Memory) network. This is especially suitable for time-series data like workout history.\\n\\nFirst, let's define a sequential model with LSTM layers using TensorFlow:\\n\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\\n\\nnum_features = 5                  # Assumption\\nnum_output_classes = 8            # Assumption\\n\\n# Define the LSTM model architecture\\nmodel = Sequential([\\n    LSTM(64, return_sequences=True, input_shape=(None, num_features)),\\n    Dropout(0.5),\\n    LSTM(32),\\n    Dropout(0.5),\\n    Dense(num_output_classes, activation='softmax')\\n])\\n\\n# Compile the model with appropriate loss function and optimizer\\nmodel.compile(\\n    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']\\n    )\\n\\n# Assume num_features is the number of input features, and num_output_classes\\n# is the number of classes for the recommendation output.\\n\\n- The LSTM layers capture temporal dependencies in the user workout data.\\n- `Dropout` is applied to prevent overfitting by randomly setting inputs to zero during training.\\n- The final `Dense` layer with a `softmax` activation function is for classification, mapping the model output to different workout recommendations.\\n- The `'categorical_crossentropy'` loss function is used for multi-class classification, which aligns with the recommendation system.\\n\\nTo train the model on your dataset, you would use:\\n\\n```python\\n# Fit the model to the training data (X_train, y_train) with validation split\\nhistory = model.fit(\\n    X_train, y_train, epochs=25, batch_size=32, validation_split=0.1\\n    )\\n\\n# X_train is training data, y_train are corresponding labels or recommendations.\\n```\\n\\n- `fit` trains the model for a fixed number of epochs (iterations over the entire dataset), and `validation_split` helps monitor for overfitting.\\n\\nFor the dynamic update based on user feedback, you'll need a mechanism to input new data into the model and retrain or fine-tune it periodically.\\n\\nA practical approach to updating the training set involves appending the new user feedback data to the existing training set and making sure the model continues learning without forgetting previous patterns. This is critical for the evolution of the workout plan recommendations. Let me give you an example using synthetic data:\\n\\nFirst, let's assume you have initial training data `X_train` and `y_train`. When new feedback comes in, represented as `X_new` and `y_new`, you'd append it to the existing data:\\n\\nimport numpy as np\\nfrom tensorflow.keras.utils import to_categorical\\n\\n# Synthetic existing training data\\n # e.g., 1000 samples, 10 time steps\\nX_train = np.random.random((1000, 10, num_features))\\ny_train = np.random.randint(0, num_output_classes, 1000)\\n\\n# Synthetic new user feedback data to be appended\\nX_new = np.random.random((100, 10, num_features))  # e.g., 100 new samples\\ny_new = np.random.randint(0, num_output_classes, 100)\\n\\n# Append new data to existing dataset\\nX_train_updated = np.concatenate((X_train, X_new), axis=0)\\n\\n\\n# One-hot encode the labels\\ny_train_encoded = to_categorical(y_train, num_classes=num_output_classes)\\ny_new_encoded = to_categorical(y_new, num_classes=num_output_classes)\\n\\n# Update the training set with one-hot encoded labels\\ny_train_updated = np.concatenate((y_train_encoded, y_new_encoded), axis=0)\\n\\nWith the dataset updated, here's how you'd continue to train (fine-tune) the model:\\n\\n# Fit the model to the updated training data\\nhistory = model.fit(\\n    X_train_updated,\\n    y_train_updated,\\n    epochs=10,\\n    batch_size=32,\\n    validation_split=0.1\\n    )\\n\\n- Here, `epochs` is set to a smaller number since this is a fine-tuning step.\\n- `validation_split` retains a portion of the updated data to evaluate the model's performance without using the test set just yet.\\n\\nIncrementally updating your training set in this manner allows the LSTM model to adapt to new user feedback without starting the learning process from scratch each time. This strategy strives to balance retaining what the model has learned about user preferences and fitness progress with incorporating new insights as they become available.\\n\\nUpon receiving new user feedback, you can further train the model with an extended dataset, including this new information. This keeps the model's recommendations fresh and aligned with the users' evolving fitness goals and preferences.\"},\n",
       "  {'role': 'User',\n",
       "   'content': \"After integrating the feedback loop into our training process, I've observed considerable improvements in the model's recommendations. However, as our system evolves, we need to ensure that it can scale to accommodate a growing user base. This may entail processing larger datasets and handling multiple feedback loops concurrently. Could you outline a strategy or modifications required in our TensorFlow model to efficiently manage this upscaling while maintaining performance?\"},\n",
       "  {'role': 'Assistant',\n",
       "   'content': \"**Assistant**\\n\\nTo prepare your TensorFlow model for scaling, you need to focus on efficient data handling, model execution, and the potential for distributed training. Here's a strategy that outlines the modifications and considerations for managing larger datasets and multiple concurrent feedback loops:\\n\\n1. Data Pipeline Optimization:\\n   TensorFlow's `tf.data` API allows you to build a robust input pipeline that can handle large amounts of data efficiently, which is essential as your dataset grows.\\n\\n\\nimport tensorflow as tf\\n\\n# Assume `X_train_updated`, `y_train_updated` contain all your training data\\n# Create a TensorFlow dataset object\\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\\n    (X_train_updated, y_train_updated)\\n    )\\n\\n# Apply necessary data transformations like batching, shuffling, and prefetching\\ntrain_dataset = train_dataset.shuffle(\\n    buffer_size=1024\\n    ).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\"},\n",
       "  {'role': 'User',\n",
       "   'content': \"2. Model Training Distribution:\\n   Utilize TensorFlow's distribution strategies (like `tf.distribute.MirroredStrategy`) to enable distributed training across multiple GPUs to reduce training time.\"},\n",
       "  {'role': 'Assistant',\n",
       "   'content': \"# Initialize the distribution strategy\\nstrategy = tf.distribute.MirroredStrategy()\\n\\nwith strategy.scope():\\n    # Define and compile the LSTM model within the distribution strategy scope\\n    # Define the LSTM model architecture\\n    model = Sequential([\\n        LSTM(64, return_sequences=True, input_shape=(None, num_features)),\\n        Dropout(0.5),\\n        LSTM(32),\\n        Dropout(0.5),\\n        Dense(num_output_classes, activation='softmax')\\n    ])\\n\\n    model.compile(\\n    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']\\n    )\\n\\n3. Incremental Learning:\\n   Retain the mechanism for updating the training set with new feedback as described previously to maintain the model's relevancy and adaptability.\\n\\n4. Asynchronous Training Updates:\\n   Consider employing an asynchronous approach to updating your model with a queue system for incoming feedback, ensuring that the model is continuously updated without causing training bottlenecks.\\n\\n5. Monitoring and Autoscaling:\\n   Implement monitoring of your model's metrics and set up autoscaling based on these metrics to manage resources efficiently.\\n\\nBy implementing these strategies, you'll ensure that your fitness recommendation system can handle scaling both in terms of data volume and concurrent user interactions. Always monitor system performance to make informed decisions about when to scale and by how much, to achieve a balance between resource allocation and system responsiveness.\"},\n",
       "  {'role': 'User',\n",
       "   'content': \"Our existing model has been adept at learning from workout data and feedback, and with scaling strategies in place, it's time to diversify the input data streams. Considering various factors like diet, sleep patterns, and vital signs can have a significant impact on a user's fitness journey, how would you suggest we integrate such diverse datasets into our current machine learning model to provide a more holistic recommendation?\"},\n",
       "  {'role': 'Assistant',\n",
       "   'content': \"Integrating diverse datasets into your machine learning model for a holistic recommendation involves creating a comprehensive multi-input model that can process different types of data concurrently. In TensorFlow, you can define a model with multiple input layers that correspond to different data sources. Here's a conceptual outline:\\n\\nSuppose we have three different datasets: workout data (sequential), dietary data (categorical), and sleep pattern data (numerical). We can use a Functional API to construct a multi-input model:\\n\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow.keras.layers import Input, LSTM\\nfrom tensorflow.keras.layers import Embedding, Dense, Concatenate, Flatten\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.utils import to_categorical\\n\\n# Assumptions about the feature dimensions\\nnum_workout_features = 3\\nnum_output_classes = 8  # Example: no. different workout recommendation types\\ndiet_categories = 50  # Example: number of unique diet categories\\n\\n# Assume that the input shapes are as follows:\\n# X_workout_train.shape = (num_samples, sequence_length, num_workout_features)\\n# X_diet_train.shape = (num_samples, 1) # One categorical value per sample\\n# X_sleep_train.shape = (num_samples, 1) # One numerical feature, like sleep hrs\\n\\n# Placeholder variables for training data\\nX_workout_train = np.random.random((1000, 10, num_workout_features))\\nX_diet_train = np.random.randint(0, diet_categories, size=(1000, 1))\\nX_sleep_train = np.random.random((1000, 1))\\ny_train = to_categorical(np.random.randint(\\n    0, num_output_classes, size=(1000,)\\n    ), num_classes=num_output_classes)\\n\\n# Input layers for each data stream\\nworkout_input = Input(shape=(None, num_workout_features), name='workout_input')\\ndiet_input = Input(shape=(1,), name='diet_input')\\nsleep_input = Input(shape=(1,), name='sleep_input')\\n\\n# LSTM layer for workout data (no return_sequences as we flatten output later)\\nworkout_branch = LSTM(64)(workout_input)\\n\\n# Flatten the output of the Embedding layer for diet data\\ndiet_branch = Flatten()(Embedding(\\n    input_dim=diet_categories, output_dim=8)(diet_input)\\n    )\\n\\n# Dense layer for sleep data\\nsleep_branch = Dense(16, activation='relu')(sleep_input)\\n\\n# Concatenate the outputs of all branches\\ncombined_features = Concatenate()([workout_branch, diet_branch, sleep_branch])\\n\\n# Final dense layer for predictions\\noutput = Dense(\\n    num_output_classes, activation='softmax', name='output'\\n    )(combined_features)\\n\\n# Construct the model with the functional API\\nmodel = Model(inputs=[workout_input, diet_input, sleep_input], outputs=output)\\n\\n# Compile the model with an optimizer, loss function, and metrics\\nmodel.compile(\\n    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']\\n    )\\n\\n# Train the model on the multi-input data\\nhistory = model.fit(\\n    [X_workout_train, X_diet_train, X_sleep_train],  # Input data list\\n    y_train,  # Target data\\n    epochs=25,\\n    batch_size=32,\\n    validation_split=0.1\\n)\\n\\nThe script is designed to construct and train a multi-input neural network suitable for processing and learning from diverse datasets. The key elements of the script are as follows:\\n\\n- Input layers are configured for three distinct types of data: workout sequences (`workout_input`), categorical diet information (`diet_input`), and numerical sleep quality (`sleep_input`).\\n- The workout data is processed through an LSTM layer perfect for handling time-series data, allowing the network to learn from the order and patterns of the workouts.\\n- The diet data is treated as categorical and passes through an `Embedding` layer, useful for transforming sparse categorical inputs into dense and meaningful representations.\\n- Sleep data is processed with a `Dense` layer to capture the impact of sleep duration or quality on workout effectiveness.\\n- The outputs of these separate pathways are then concatenated into a single feature vector by a `Concatenate` layer, providing a fused representation of all inputs.\\n- This combined feature vector is passed through a final `Dense` layer with a `softmax` activation function to generate the output recommendations. It classifies the input data into one of several categories representing different workout plans.\\n- We compile the model with the Adam optimizer and categorical crossentropy loss function, aligning with our multi-class problem.\\n- Finally, the model is trained using the `fit` method on our prepared datasets for workouts, diet, and sleep (`X_workout_train`, `X_diet_train`, `X_sleep_train`), and corresponding targets (`y_train`), with the number of epochs and batch size specified.\\n\\nThe training is done with a validation split to monitor performance on unseen data during training, providing an indication of how the model generalizes beyond the training set. The model's architecture and training process are crafted to optimize the use of varied data types, essential for an integrated approach to generating personalized and comprehensive fitness recommendations.\\n\\nThis way of integrating diverse datasets into a unified model allows for comprehensive analysis and nuanced recommendations that consider a broader scope of user behaviors and wellness factors, providing users with a well-rounded approach to fitness.\"}],\n",
       " 'id': '1ztZJScju_m-9MYF2v1fze2TVgjPadgNF'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_link</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>number_of_turns</th>\n",
       "      <th>gpt_estimated_duration</th>\n",
       "      <th>action</th>\n",
       "      <th>domain__top_level</th>\n",
       "      <th>domain__sub_level</th>\n",
       "      <th>use_case__summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://colab.research.google.com/drive/1ztZJS...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Write end to end ML training code</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Neural Network Design</td>\n",
       "      <td>The User seeks to enhance a fitness recommenda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://colab.research.google.com/drive/1QBLfW...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Write end to end ML training code</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Neural Networks / Deep Learning</td>\n",
       "      <td>The User is using the Assistant to enhance a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://colab.research.google.com/drive/1OiW7x...</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Do a code review</td>\n",
       "      <td>Python basics &amp; scripting</td>\n",
       "      <td>Code Review and Best Practices</td>\n",
       "      <td>User seeks assistance with Python script revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://colab.research.google.com/drive/1pzywt...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Debug error trace</td>\n",
       "      <td>Python basics &amp; scripting</td>\n",
       "      <td>Error Handling</td>\n",
       "      <td>User seeks assistance for a coding error, usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://colab.research.google.com/drive/1JJuMy...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Help me take an interview</td>\n",
       "      <td>Interview Prep</td>\n",
       "      <td>Mock Interview for Web Development</td>\n",
       "      <td>User seeks mock interview practice for buildin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963</th>\n",
       "      <td>https://colab.research.google.com/drive/1X-6J-...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Answer infra questions</td>\n",
       "      <td>Web Development</td>\n",
       "      <td>HTTPS Configuration</td>\n",
       "      <td>User seeks assistance for enabling HTTPS on a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>https://colab.research.google.com/drive/1cx0m_...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Write unit tests</td>\n",
       "      <td>Testing</td>\n",
       "      <td>Integration Testing</td>\n",
       "      <td>User seeks assistance with integration testing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3965</th>\n",
       "      <td>https://colab.research.google.com/drive/1_HLk3...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Explain code</td>\n",
       "      <td>Python basics &amp; scripting</td>\n",
       "      <td>Serialization</td>\n",
       "      <td>User seeks assistance with Python serializatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3966</th>\n",
       "      <td>https://colab.research.google.com/drive/1lTlIm...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Write code in python</td>\n",
       "      <td>Problem Solving</td>\n",
       "      <td>Algorithm Explanation</td>\n",
       "      <td>User seeks understanding of merge sort and its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967</th>\n",
       "      <td>https://colab.research.google.com/drive/18ETs1...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Write code in python</td>\n",
       "      <td>Python basics &amp; scripting</td>\n",
       "      <td>Data manipulation</td>\n",
       "      <td>The user seeks assistance with Python coding t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3968 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              task_link batch_id  \\\n",
       "0     https://colab.research.google.com/drive/1ztZJS...        7   \n",
       "1     https://colab.research.google.com/drive/1QBLfW...        7   \n",
       "2     https://colab.research.google.com/drive/1OiW7x...            \n",
       "3     https://colab.research.google.com/drive/1pzywt...        7   \n",
       "4     https://colab.research.google.com/drive/1JJuMy...        7   \n",
       "...                                                 ...      ...   \n",
       "3963  https://colab.research.google.com/drive/1X-6J-...        2   \n",
       "3964  https://colab.research.google.com/drive/1cx0m_...        3   \n",
       "3965  https://colab.research.google.com/drive/1_HLk3...        4   \n",
       "3966  https://colab.research.google.com/drive/1lTlIm...        4   \n",
       "3967  https://colab.research.google.com/drive/18ETs1...        3   \n",
       "\n",
       "     number_of_turns gpt_estimated_duration  \\\n",
       "0                  4                   60.0   \n",
       "1                  4                   60.0   \n",
       "2                  3                   30.0   \n",
       "3                  2                   10.0   \n",
       "4                  4                   30.0   \n",
       "...              ...                    ...   \n",
       "3963               2                   10.0   \n",
       "3964               2                   30.0   \n",
       "3965               3                   10.0   \n",
       "3966               3                   10.0   \n",
       "3967               4                   15.0   \n",
       "\n",
       "                                 action          domain__top_level  \\\n",
       "0     Write end to end ML training code           Machine Learning   \n",
       "1     Write end to end ML training code           Machine Learning   \n",
       "2                      Do a code review  Python basics & scripting   \n",
       "3                     Debug error trace  Python basics & scripting   \n",
       "4             Help me take an interview             Interview Prep   \n",
       "...                                 ...                        ...   \n",
       "3963             Answer infra questions            Web Development   \n",
       "3964                   Write unit tests                    Testing   \n",
       "3965                       Explain code  Python basics & scripting   \n",
       "3966               Write code in python            Problem Solving   \n",
       "3967               Write code in python  Python basics & scripting   \n",
       "\n",
       "                       domain__sub_level  \\\n",
       "0                  Neural Network Design   \n",
       "1        Neural Networks / Deep Learning   \n",
       "2         Code Review and Best Practices   \n",
       "3                         Error Handling   \n",
       "4     Mock Interview for Web Development   \n",
       "...                                  ...   \n",
       "3963                 HTTPS Configuration   \n",
       "3964                 Integration Testing   \n",
       "3965                       Serialization   \n",
       "3966               Algorithm Explanation   \n",
       "3967                   Data manipulation   \n",
       "\n",
       "                                      use_case__summary  \n",
       "0     The User seeks to enhance a fitness recommenda...  \n",
       "1     The User is using the Assistant to enhance a f...  \n",
       "2     User seeks assistance with Python script revie...  \n",
       "3     User seeks assistance for a coding error, usin...  \n",
       "4     User seeks mock interview practice for buildin...  \n",
       "...                                                 ...  \n",
       "3963  User seeks assistance for enabling HTTPS on a ...  \n",
       "3964  User seeks assistance with integration testing...  \n",
       "3965  User seeks assistance with Python serializatio...  \n",
       "3966  User seeks understanding of merge sort and its...  \n",
       "3967  The user seeks assistance with Python coding t...  \n",
       "\n",
       "[3968 rows x 8 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    477.000000\n",
       "mean      24.643606\n",
       "std       16.493532\n",
       "min        5.000000\n",
       "25%       10.000000\n",
       "50%       30.000000\n",
       "75%       30.000000\n",
       "max       60.000000\n",
       "Name: gpt_estimated_duration, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insights_df[insights_df[\"batch_id\"]==\"7\"][\"gpt_estimated_duration\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated or appended data to 'v1 (Jan 25)'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1v_O33STdi_h7taPd3MkD0fiqRx7rqr_aAQWGnlOfr_w',\n",
       " 'tableRange': \"'v1 (Jan 25)'!A1:G975\",\n",
       " 'updates': {'spreadsheetId': '1v_O33STdi_h7taPd3MkD0fiqRx7rqr_aAQWGnlOfr_w',\n",
       "  'updatedRange': \"'v1 (Jan 25)'!A976:H1950\",\n",
       "  'updatedRows': 975,\n",
       "  'updatedColumns': 8,\n",
       "  'updatedCells': 7800}}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is it safe to upload the files to the insights sheet? how many will we lose? yes\n",
    "\n",
    "\n",
    "# Merge and upload the GPT estimated durations\n",
    "\n",
    "# Download the insights sheet\n",
    "# existing_insights_v1 = download_sheet_as_df(service_account_file, insights_sheet_id, INSIGHTS_VERSION_TAB)\n",
    "\n",
    "# Merge the new insights with the existing ones\n",
    "# merged_insights = existing_insights_v1.merge(insights_df[[\"task_link\", \"gpt_estimated_duration\"]], on=\"task_link\", how=\"left\")\n",
    "\n",
    "\n",
    "from src.sheets_utils import upload_df_to_sheet, GoogleSheetsService\n",
    "\n",
    "\n",
    "sheets_client = GoogleSheetsService(service_account_file, ['https://www.googleapis.com/auth/spreadsheets'])\n",
    "merged_insights = merged_insights.fillna(\"\")\n",
    "values = [merged_insights.columns.tolist()] + merged_insights.values.tolist()\n",
    "sheets_client.update_or_append_data_to_sheet(insights_sheet_id, INSIGHTS_VERSION_TAB, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated or appended data to 'v1 (Jan 25)'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1v_O33STdi_h7taPd3MkD0fiqRx7rqr_aAQWGnlOfr_w',\n",
       " 'tableRange': \"'v1 (Jan 25)'!A1:H975\",\n",
       " 'updates': {'spreadsheetId': '1v_O33STdi_h7taPd3MkD0fiqRx7rqr_aAQWGnlOfr_w',\n",
       "  'updatedRange': \"'v1 (Jan 25)'!A976:H3985\",\n",
       "  'updatedRows': 3010,\n",
       "  'updatedColumns': 8,\n",
       "  'updatedCells': 24080}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheets_client = GoogleSheetsService(service_account_file, ['https://www.googleapis.com/auth/spreadsheets'])\n",
    "insights_df = insights_df.fillna(\"\")\n",
    "insights_df_old = insights_df[~insights_df[\"batch_id\"].isin([\"7\", \"6\"])]\n",
    "values = [insights_df_old.columns.tolist()] + insights_df_old.values.tolist()\n",
    "sheets_client.update_or_append_data_to_sheet(insights_sheet_id, INSIGHTS_VERSION_TAB, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_link</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>number_of_turns</th>\n",
       "      <th>gpt_estimated_duration</th>\n",
       "      <th>action</th>\n",
       "      <th>domain__top_level</th>\n",
       "      <th>domain__sub_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://colab.research.google.com/drive/1ztZJS...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Write end to end ML training code</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Neural Network Design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://colab.research.google.com/drive/1QBLfW...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Write end to end ML training code</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Neural Networks / Deep Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://colab.research.google.com/drive/1OiW7x...</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Do a code review</td>\n",
       "      <td>Python basics &amp; scripting</td>\n",
       "      <td>Code Review and Best Practices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://colab.research.google.com/drive/1pzywt...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Debug error trace</td>\n",
       "      <td>Python basics &amp; scripting</td>\n",
       "      <td>Error Handling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://colab.research.google.com/drive/1JJuMy...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Help me take an interview</td>\n",
       "      <td>Interview Prep</td>\n",
       "      <td>Mock Interview for Web Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963</th>\n",
       "      <td>https://colab.research.google.com/drive/1X-6J-...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Answer infra questions</td>\n",
       "      <td>Web Development</td>\n",
       "      <td>HTTPS Configuration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>https://colab.research.google.com/drive/1cx0m_...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Write unit tests</td>\n",
       "      <td>Testing</td>\n",
       "      <td>Integration Testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3965</th>\n",
       "      <td>https://colab.research.google.com/drive/1_HLk3...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Explain code</td>\n",
       "      <td>Python basics &amp; scripting</td>\n",
       "      <td>Serialization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3966</th>\n",
       "      <td>https://colab.research.google.com/drive/1lTlIm...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Write code in python</td>\n",
       "      <td>Problem Solving</td>\n",
       "      <td>Algorithm Explanation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3967</th>\n",
       "      <td>https://colab.research.google.com/drive/18ETs1...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Write code in python</td>\n",
       "      <td>Python basics &amp; scripting</td>\n",
       "      <td>Data manipulation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3968 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              task_link batch_id  \\\n",
       "0     https://colab.research.google.com/drive/1ztZJS...        7   \n",
       "1     https://colab.research.google.com/drive/1QBLfW...        7   \n",
       "2     https://colab.research.google.com/drive/1OiW7x...            \n",
       "3     https://colab.research.google.com/drive/1pzywt...        7   \n",
       "4     https://colab.research.google.com/drive/1JJuMy...        7   \n",
       "...                                                 ...      ...   \n",
       "3963  https://colab.research.google.com/drive/1X-6J-...        2   \n",
       "3964  https://colab.research.google.com/drive/1cx0m_...        3   \n",
       "3965  https://colab.research.google.com/drive/1_HLk3...        4   \n",
       "3966  https://colab.research.google.com/drive/1lTlIm...        4   \n",
       "3967  https://colab.research.google.com/drive/18ETs1...        3   \n",
       "\n",
       "     number_of_turns gpt_estimated_duration  \\\n",
       "0                  4                   60.0   \n",
       "1                  4                   60.0   \n",
       "2                  3                   30.0   \n",
       "3                  2                   10.0   \n",
       "4                  4                   30.0   \n",
       "...              ...                    ...   \n",
       "3963               2                   10.0   \n",
       "3964               2                   30.0   \n",
       "3965               3                   10.0   \n",
       "3966               3                   10.0   \n",
       "3967               4                   15.0   \n",
       "\n",
       "                                 action          domain__top_level  \\\n",
       "0     Write end to end ML training code           Machine Learning   \n",
       "1     Write end to end ML training code           Machine Learning   \n",
       "2                      Do a code review  Python basics & scripting   \n",
       "3                     Debug error trace  Python basics & scripting   \n",
       "4             Help me take an interview             Interview Prep   \n",
       "...                                 ...                        ...   \n",
       "3963             Answer infra questions            Web Development   \n",
       "3964                   Write unit tests                    Testing   \n",
       "3965                       Explain code  Python basics & scripting   \n",
       "3966               Write code in python            Problem Solving   \n",
       "3967               Write code in python  Python basics & scripting   \n",
       "\n",
       "                       domain__sub_level  \n",
       "0                  Neural Network Design  \n",
       "1        Neural Networks / Deep Learning  \n",
       "2         Code Review and Best Practices  \n",
       "3                         Error Handling  \n",
       "4     Mock Interview for Web Development  \n",
       "...                                  ...  \n",
       "3963                 HTTPS Configuration  \n",
       "3964                 Integration Testing  \n",
       "3965                       Serialization  \n",
       "3966               Algorithm Explanation  \n",
       "3967                   Data manipulation  \n",
       "\n",
       "[3968 rows x 7 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3921, 3969)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def is_metadata_v1(file_contents):\n",
    "    \"\"\"\n",
    "    Verify the schema.\n",
    "\n",
    "    Includes metadata dict, which includes these keys\n",
    "    - domain\n",
    "    - action\n",
    "    - use_case_summary\n",
    "    - gpt_estimated_duration\n",
    "    - task_link\n",
    "    \"\"\"\n",
    "    if file_contents is None:\n",
    "        return False\n",
    "\n",
    "    if \"metadata\" in file_contents.keys():\n",
    "        if \"domain\" in file_contents[\"metadata\"] and \\\n",
    "            \"action\" in file_contents[\"metadata\"] and \\\n",
    "            \"use_case_summary\" in file_contents[\"metadata\"] and \\\n",
    "            \"gpt_estimated_duration\" in file_contents[\"metadata\"] and \\\n",
    "            \"task_link\" in file_contents[\"metadata\"]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# count how many elements of the results are v1 schema\n",
    "count = 0\n",
    "for r in results:\n",
    "    if is_metadata_v1(r):\n",
    "        count += 1\n",
    "\n",
    "count, len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'task_link': 'https://colab.research.google.com/drive/1J4ftUr1iSgQOXjaHehIsp9Sxs-0tF9qG.json',\n",
       "  'number_of_turns': '2',\n",
       "  'use_case__summary': 'The user is using the Assistant to enhance web development security practices.',\n",
       "  'area_of_focus__detailed_level': 'secure cookies in flask',\n",
       "  'domain__detailed_level': 'security and authentication',\n",
       "  'behavioral_tags': [{'top_level': 'continuation follow up',\n",
       "    'sub_level': 'request for clarification/elaboration',\n",
       "    'custom_category': 'FALSE'},\n",
       "   {'top_level': 'respond to assistant',\n",
       "    'sub_level': 'answer a question',\n",
       "    'custom_category': 'FALSE'},\n",
       "   {'top_level': 'continuation follow up',\n",
       "    'sub_level': 'incrementally build',\n",
       "    'custom_category': 'FALSE'}],\n",
       "  'programming_language_tags': [{'language': 'python', 'percentage': '1'}],\n",
       "  'dependency_tags': [{'dependency': 'flask', 'percentage': '1'}],\n",
       "  'topic_classication': 'web_development > web_security',\n",
       "  'area_of_focus_classification': {'top_level': 'general coding help',\n",
       "   'sub_level': 'explain complex code snippets'},\n",
       "  'domain_classification': {'top_level': 'web development',\n",
       "   'sub_level': 'backend development'},\n",
       "  'domain': {'top_level': 'Web Development', 'sub_level': 'Security'},\n",
       "  'action': 'Write code in python',\n",
       "  'use_case_summary': 'User seeks assistance for secure cookie handling in Flask and SSL certificate setup for development.',\n",
       "  'gpt_estimated_duration': 10},\n",
       " 'messages': [{'role': 'User',\n",
       "   'content': \"I'm storing session tokens in cookies with Flask, but I've heard that setting the 'secure' flag on cookies is a good practice. How do I modify the set_cookie function to include this?\"},\n",
       "  {'role': 'Assistant',\n",
       "   'content': \"In Flask, when you use the `set_cookie` function as part of the response object to store session tokens or any cookies, you can include the `secure` flag to ensure that the cookie is only sent over HTTPS connections.\\n\\nHere's an example of how you can modify the `set_cookie` function to include the `secure` flag:\\n\\n```python\\nfrom flask import Flask, make_response\\n\\napp = Flask(__name__)\\n\\n@app.route('/set_cookie')\\ndef set_secure_cookie():\\n    response = make_response('Setting a secure cookie')\\n    response.set_cookie('session_token', value='your_session_token_here', secure=True, httponly=True)\\n    return response\\n\\nif __name__ == '__main__':\\n    app.run(ssl_context='adhoc')  # This will run the app with an ad-hoc SSL context for HTTPS\\n```\\n\\nIn the above code:\\n- The `secure=True` parameter in Flask's `set_cookie` function ensures cookies are sent over HTTPS for security.\\n- `httponly=True` prevents access to the cookie through JavaScript, protecting against cross-site scripting (XSS) attacks.\\n\\nFor the secure flag to be effective, the app must use HTTPS, which can be set up in Flask for testing purposes with `app.run(ssl_context='adhoc')`. In production, a valid SSL certificate should be used instead.\"},\n",
       "  {'role': 'User',\n",
       "   'content': 'What steps do I need to follow to setup an ssl certificate for development?'},\n",
       "  {'role': 'Assistant',\n",
       "   'content': \"Setting up a valid SSL certificate in a production environment for a Flask application typically involves the following steps:\\n\\n1. **Obtain SSL Certificate**: You first need to obtain an SSL certificate issued by a Certificate Authority (CA). You can use Let's Encrypt, a free, automated, and open CA, to get a certificate or buy one from another provider.\\n\\n2. **Install SSL Certificate**: Once you have your SSL certificate (usually a `.crt` file) and your private key (usually a `.key` file), you need to install them on your server. The specifics of this step depend on your hosting setup and server software.\\n\\n3. **Configure Web Server**: Flask's built-in server is not suitable for production. You should deploy your Flask application behind a reliable web server like Nginx or Apache, which will serve as a reverse proxy to your Flask application.\\n\\n4. **Enable HSTS**: It's recommended to enable HTTP Strict Transport Security (HSTS) by adding the appropriate header (`Strict-Transport-Security`) to tell browsers to always use HTTPS for your domain.\\n\\n5. **Keep SSL Updated**: If you are using a service like Let's Encrypt, you need to automate the renewal process of your SSL certificates since they typically expire every 90 days.\\n\\n6. **Testing**: After setting everything up, test your SSL configuration using tools like SSL Labs' SSL Test to ensure it's secure and correctly configured\"}],\n",
       " 'id': '1J4ftUr1iSgQOXjaHehIsp9Sxs-0tF9qG'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpublish Polluted Projects / Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding 56 tasks\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 400 when requesting https://sheets.googleapis.com/v4/spreadsheets/1eUif5I8xhHU8fY0X9v8r2JI9hWPh7Dq_9VXpSIHwww4/values/%27v1%20%28Jan%2025%29%27%21A%3AH?alt=json returned \"Unable to parse range: 'v1 (Jan 25)'!A:H\". Details: \"Unable to parse range: 'v1 (Jan 25)'!A:H\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msheets_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleSheetsService\n\u001b[1;32m     23\u001b[0m sheets_client \u001b[38;5;241m=\u001b[39m GoogleSheetsService(service_account_file, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/auth/spreadsheets\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 24\u001b[0m \u001b[43msheets_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_rows_in_list_of_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelivery_sheet_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINSIGHTS_VERSION_TAB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_exclude_tasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/turing/character.ai/character_tasks/operations/delivery/../../src/sheets_utils.py:86\u001b[0m, in \u001b[0;36mGoogleSheetsService.delete_rows_in_list_of_values\u001b[0;34m(self, spreadsheet_id, sheet_title, list_of_values)\u001b[0m\n\u001b[1;32m     84\u001b[0m cleaned_sheet_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msheet_title\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m range_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcleaned_sheet_title\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!A:H\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# This is the variable that probably caused the crash\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspreadsheets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspreadsheetId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspreadsheet_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrange_\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m current_values \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m, [])\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Find the rows to delete\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://sheets.googleapis.com/v4/spreadsheets/1eUif5I8xhHU8fY0X9v8r2JI9hWPh7Dq_9VXpSIHwww4/values/%27v1%20%28Jan%2025%29%27%21A%3AH?alt=json returned \"Unable to parse range: 'v1 (Jan 25)'!A:H\". Details: \"Unable to parse range: 'v1 (Jan 25)'!A:H\">"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../../')\n",
    "from src.sheets_utils import download_sheet_as_df\n",
    "\n",
    "# Download the insights sheet\n",
    "metadata_df = download_sheet_as_df(\n",
    "    service_account_file,\n",
    "    insights_sheet_id,\n",
    "    \"v1 (Jan 25)\"\n",
    ")\n",
    "\n",
    "\n",
    "# Filter for subset (batch_id == 10 && action in [\"scrape a website\", \"Help me take an Interview\", \"beam code\", \"spark code\"])\n",
    "to_exclude_tasks = metadata_df[\n",
    "    (metadata_df[\"batch_id\"] == \"10\") & \n",
    "    (metadata_df[\"action\"].isin([\"Scrape a website\", \"Help me take an interview\", \"Write / modify / fix beam code\", \"Write / modify / fix spark code\"]))\n",
    "][\"task_link\"].to_list()\n",
    "print(f\"Excluding {len(to_exclude_tasks)} tasks\")\n",
    "\n",
    "# Remove these task_links from delivery sheet, inisghts sheet, and gdrive folder\n",
    "from src.sheets_utils import GoogleSheetsService\n",
    "\n",
    "sheets_client = GoogleSheetsService(service_account_file, ['https://www.googleapis.com/auth/spreadsheets'])\n",
    "sheets_client.delete_rows_in_list_of_values(delivery_sheet_id, INSIGHTS_VERSION_TAB, to_exclude_tasks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "# Function to setup the gspread client\n",
    "def google_sheet_auth(json_file_path):\n",
    "    scope = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "    credentials = Credentials.from_service_account_file(json_file_path, scopes=scope)\n",
    "    client = gspread.authorize(credentials)\n",
    "    return client\n",
    "\n",
    "# Main function to delete rows with a given list of values in a column\n",
    "def delete_rows_from_google_sheet(json_file_path, sheet_url, tab_name, search_column_index, values_to_delete):\n",
    "    # Authenticate with the Google Sheet\n",
    "    gspread_client = google_sheet_auth(json_file_path)\n",
    "    google_sheet = gspread_client.open_by_url(sheet_url)\n",
    "    worksheet = google_sheet.worksheet(tab_name) \n",
    "\n",
    "    # Extracting all data from the target column\n",
    "    all_values = worksheet.col_values(search_column_index)\n",
    "    rows_to_delete = [i for i, item in enumerate(all_values, 1) if item in values_to_delete]\n",
    "    \n",
    "    # Gspread doesn't support direct deletion of multiple rows in arbitrary order.\n",
    "    # The loop will delete in the reverse order to avoid index shifting problems.\n",
    "    for i in sorted(rows_to_delete, reverse=True):\n",
    "        worksheet.delete_rows(i)\n",
    "\n",
    "# Example variables\n",
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1v_O33STdi_h7taPd3MkD0fiqRx7rqr_aAQWGnlOfr_w/edit#gid=1753316694'\n",
    "tab_name = 'v1 (Jan 25)'\n",
    "search_column_index = 1  # Considering first column as an example\n",
    "\n",
    "# Delete operation\n",
    "delete_rows_from_google_sheet(service_account_file, sheet_url, tab_name, search_column_index, to_exclude_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example variables\n",
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1eUif5I8xhHU8fY0X9v8r2JI9hWPh7Dq_9VXpSIHwww4/edit#gid=281973205'\n",
    "tab_name = 'Batch 10'\n",
    "search_column_index = 1  # Considering first column as an example\n",
    "\n",
    "# Delete operation\n",
    "delete_rows_from_google_sheet(service_account_file, sheet_url, tab_name, search_column_index, to_exclude_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gspread\n",
      "  Downloading gspread-6.0.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 KB\u001b[0m \u001b[31m830.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth>=1.12.0 in /home/joe96/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages (from gspread) (2.25.2)\n",
      "Collecting StrEnum==0.4.15\n",
      "  Using cached StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Collecting google-auth-oauthlib>=0.4.1\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/joe96/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages (from google-auth>=1.12.0->gspread) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/joe96/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages (from google-auth>=1.12.0->gspread) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/joe96/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages (from google-auth>=1.12.0->gspread) (5.3.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/joe96/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.5.1)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/joe96/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/joe96/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/joe96/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/joe96/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/joe96/projects/turing/character.ai/character_tasks/venv/lib/python3.9/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.6)\n",
      "Installing collected packages: StrEnum, oauthlib, requests-oauthlib, google-auth-oauthlib, gspread\n",
      "Successfully installed StrEnum-0.4.15 google-auth-oauthlib-1.2.0 gspread-6.0.2 oauthlib-3.2.2 requests-oauthlib-1.3.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/home/joe96/projects/turing/character.ai/character_tasks/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://colab.research.google.com/drive/1ISirhbmPOg8hMIJuvIHVtv-MymPC13cO',\n",
       " 'https://colab.research.google.com/drive/1ca91-ZE2u1uigCJgT-da-fypBEtzYNvB',\n",
       " 'https://colab.research.google.com/drive/1uU28jLtLHaZM63YQf2HAUqlHm76Y84Uu',\n",
       " 'https://colab.research.google.com/drive/1_slXeCkFcpl5tT_bh1SnFzIz3qPSNQjW',\n",
       " 'https://colab.research.google.com/drive/1F8pDEDA3UzOzRmt2fK3DTUBXQWFeMNrv',\n",
       " 'https://colab.research.google.com/drive/1kv9kfPyGyjvY1O1UH_01iLctxfS1UGk9',\n",
       " 'https://colab.research.google.com/drive/1zffjzl7-PSmyfljmyWzp98JHSuvbf1Mz',\n",
       " 'https://colab.research.google.com/drive/10MGY0KFzDDatqxqAAnb_Qr3QERkrIt76',\n",
       " 'https://colab.research.google.com/drive/1hBGIIO1iAs_TOcl-D22I48dmXTYs36zs',\n",
       " 'https://colab.research.google.com/drive/1ogdDTsrSk7Puy8HvD5HwTN4Lx2rxRYDx',\n",
       " 'https://colab.research.google.com/drive/1xtWX10-Qg6e0EHglgMpDWp0NARwIisYH',\n",
       " 'https://colab.research.google.com/drive/1zD0DbXGuzDmuuKWFKurLk5hp4-YHgzJt',\n",
       " 'https://colab.research.google.com/drive/1lBqPQrCdJma67bajak4aDA4ig5AHprL4',\n",
       " 'https://colab.research.google.com/drive/14VKzIcDY0jPyZj5q8U-3ji5Uc5JsqFfh',\n",
       " 'https://colab.research.google.com/drive/1GmwZRUz9Owkf2TclEkiiq3s58YCekLnd',\n",
       " 'https://colab.research.google.com/drive/1-lWGDxx41n9sg87yIsqVTur1ZVpR0nSO',\n",
       " 'https://colab.research.google.com/drive/1AM3BNP3qoqo_nHZMl0RZKQffIi62wBPQ',\n",
       " 'https://colab.research.google.com/drive/1e6ORvNVBNteC_5q2IhnfdReEdsn_bb_-',\n",
       " 'https://colab.research.google.com/drive/1j1MYrC5QM0lMjKqyCdEzyZZEEotFjl-f',\n",
       " 'https://colab.research.google.com/drive/1Jv96_tXn4GhS1SzDFGSoil7hydrHiIvj',\n",
       " 'https://colab.research.google.com/drive/1BR_cMikfmC48VdgJCGTX_-Wtl6LsJWUv',\n",
       " 'https://colab.research.google.com/drive/19bAuEuQhKrVwuCYprslbAPITShNN5A24',\n",
       " 'https://colab.research.google.com/drive/1RYoBPEVpVFvUO-uLDTWx2vckSq73Ihp-',\n",
       " 'https://colab.research.google.com/drive/1xGEHflfPWwOrHG16aSmca7I77zOtsCwr',\n",
       " 'https://colab.research.google.com/drive/1FAzFYv-fOH5v-IrxH2nn_f9n2PrfpNjl',\n",
       " 'https://colab.research.google.com/drive/1Qam-Yq2JidOmOm9pvQRR-d62w5IHLlkZ',\n",
       " 'https://colab.research.google.com/drive/1wx_UisDlDKa4LQKr99z2X0yDtFBRHs_F',\n",
       " 'https://colab.research.google.com/drive/1SbSrys_ssUfWkIAjva6dTHWHfKtKnEHh',\n",
       " 'https://colab.research.google.com/drive/1h7qcHGK0u13_ZSo5C-Zf6DWfDkkIAUCB',\n",
       " 'https://colab.research.google.com/drive/15Yz7fwUf4EuKwh_Pi6ugC6lSNEZwDRmV',\n",
       " 'https://colab.research.google.com/drive/1L-Vx6dHcuOZDgtTOrk9k9AvOgYCum6dQ',\n",
       " 'https://colab.research.google.com/drive/1KeDmjC7XFRK5mX9OXHTjGMbVzViRUOEG',\n",
       " 'https://colab.research.google.com/drive/1oru8cEwM3EEZJqafKnSp0-FY-OG16C7_',\n",
       " 'https://colab.research.google.com/drive/12xQXXUXkApDNqvwXZKGJGPsCNbxnbvcR',\n",
       " 'https://colab.research.google.com/drive/1my4v1sJ954TLesO7CesaeQ-43Y9iizqB',\n",
       " 'https://colab.research.google.com/drive/19HmT3nrmY465GRcrME91k550Km9qpB9O',\n",
       " 'https://colab.research.google.com/drive/1KGK4DybzhOU3abJofcNOjT3nTh0ezzPy',\n",
       " 'https://colab.research.google.com/drive/1HuIQZClE7GAA-Qx-bgfYJeD-cEz7x69Z',\n",
       " 'https://colab.research.google.com/drive/15eYkWuWKMfAJF5yntXj7FB-0fVdjes6s',\n",
       " 'https://colab.research.google.com/drive/1OXvNXnZoRS1Px4NCsSULspE5hdlHmD81',\n",
       " 'https://colab.research.google.com/drive/1LEJxDd8mDp5qgM7fU4HN4OlUZc3LPOLg',\n",
       " 'https://colab.research.google.com/drive/1LLfohSnpQkrwWmnWn5Z870--iAEfNi8q',\n",
       " 'https://colab.research.google.com/drive/12Hy8zETbqkppKYsjGx4unURVJzT5yOre',\n",
       " 'https://colab.research.google.com/drive/1UsbsUJfmYjAYMT0gx6Vf31E-rHgdfN6v',\n",
       " 'https://colab.research.google.com/drive/1THjPNauR474s_9_KKCLNLJMCxdjazStr',\n",
       " 'https://colab.research.google.com/drive/1zTNyAMz3-MEL-v22KhoIPrh-fkr8QO4c',\n",
       " 'https://colab.research.google.com/drive/11j9ogx3XDIe_oHPNSrXLdo-ZRd9gTBj_',\n",
       " 'https://colab.research.google.com/drive/19cEqh_5Qs5XhqBpMWB7NUlK6xdJzz0Nl',\n",
       " 'https://colab.research.google.com/drive/1H1PuQtV_iMis0A2JYyGkhDK_yO86NWf9',\n",
       " 'https://colab.research.google.com/drive/1HK6UAoy6OVsvVRuuVR6g2Q2naeSfu2so',\n",
       " 'https://colab.research.google.com/drive/1osS7hdnErgBpMhVOo_pPYA23KheFjp3-',\n",
       " 'https://colab.research.google.com/drive/19rNQcINThr752-wOdnLfYMG_GZI-G1WA',\n",
       " 'https://colab.research.google.com/drive/1rFq481ihhQ9wM2w-VVrj6pfvIah9oZFL',\n",
       " 'https://colab.research.google.com/drive/1yrFXe0ydN1jIrxFUF8ABBHFq18kAb3wU',\n",
       " 'https://colab.research.google.com/drive/1Pp8oadtFk89wvdZFFtFyHr6OejVExjXE',\n",
       " 'https://colab.research.google.com/drive/1o7aEZrKPFuFCYhI4b2cNMjjiYveqXJ_9']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_exclude_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new histroical redo sheet\n",
    "\n",
    "\n",
    "to_exclude_tasks = ['https://colab.research.google.com/drive/1ISirhbmPOg8hMIJuvIHVtv-MymPC13cO',\n",
    " 'https://colab.research.google.com/drive/1ca91-ZE2u1uigCJgT-da-fypBEtzYNvB',\n",
    " 'https://colab.research.google.com/drive/1uU28jLtLHaZM63YQf2HAUqlHm76Y84Uu',\n",
    " 'https://colab.research.google.com/drive/1_slXeCkFcpl5tT_bh1SnFzIz3qPSNQjW',\n",
    " 'https://colab.research.google.com/drive/1F8pDEDA3UzOzRmt2fK3DTUBXQWFeMNrv',\n",
    " 'https://colab.research.google.com/drive/1kv9kfPyGyjvY1O1UH_01iLctxfS1UGk9',\n",
    " 'https://colab.research.google.com/drive/1zffjzl7-PSmyfljmyWzp98JHSuvbf1Mz',\n",
    " 'https://colab.research.google.com/drive/10MGY0KFzDDatqxqAAnb_Qr3QERkrIt76',\n",
    " 'https://colab.research.google.com/drive/1hBGIIO1iAs_TOcl-D22I48dmXTYs36zs',\n",
    " 'https://colab.research.google.com/drive/1ogdDTsrSk7Puy8HvD5HwTN4Lx2rxRYDx',\n",
    " 'https://colab.research.google.com/drive/1xtWX10-Qg6e0EHglgMpDWp0NARwIisYH',\n",
    " 'https://colab.research.google.com/drive/1zD0DbXGuzDmuuKWFKurLk5hp4-YHgzJt',\n",
    " 'https://colab.research.google.com/drive/1lBqPQrCdJma67bajak4aDA4ig5AHprL4',\n",
    " 'https://colab.research.google.com/drive/14VKzIcDY0jPyZj5q8U-3ji5Uc5JsqFfh',\n",
    " 'https://colab.research.google.com/drive/1GmwZRUz9Owkf2TclEkiiq3s58YCekLnd',\n",
    " 'https://colab.research.google.com/drive/1-lWGDxx41n9sg87yIsqVTur1ZVpR0nSO',\n",
    " 'https://colab.research.google.com/drive/1AM3BNP3qoqo_nHZMl0RZKQffIi62wBPQ',\n",
    " 'https://colab.research.google.com/drive/1e6ORvNVBNteC_5q2IhnfdReEdsn_bb_-',\n",
    " 'https://colab.research.google.com/drive/1j1MYrC5QM0lMjKqyCdEzyZZEEotFjl-f',\n",
    " 'https://colab.research.google.com/drive/1Jv96_tXn4GhS1SzDFGSoil7hydrHiIvj',\n",
    " 'https://colab.research.google.com/drive/1BR_cMikfmC48VdgJCGTX_-Wtl6LsJWUv',\n",
    " 'https://colab.research.google.com/drive/19bAuEuQhKrVwuCYprslbAPITShNN5A24',\n",
    " 'https://colab.research.google.com/drive/1RYoBPEVpVFvUO-uLDTWx2vckSq73Ihp-',\n",
    " 'https://colab.research.google.com/drive/1xGEHflfPWwOrHG16aSmca7I77zOtsCwr',\n",
    " 'https://colab.research.google.com/drive/1FAzFYv-fOH5v-IrxH2nn_f9n2PrfpNjl',\n",
    " 'https://colab.research.google.com/drive/1Qam-Yq2JidOmOm9pvQRR-d62w5IHLlkZ',\n",
    " 'https://colab.research.google.com/drive/1wx_UisDlDKa4LQKr99z2X0yDtFBRHs_F',\n",
    " 'https://colab.research.google.com/drive/1SbSrys_ssUfWkIAjva6dTHWHfKtKnEHh',\n",
    " 'https://colab.research.google.com/drive/1h7qcHGK0u13_ZSo5C-Zf6DWfDkkIAUCB',\n",
    " 'https://colab.research.google.com/drive/15Yz7fwUf4EuKwh_Pi6ugC6lSNEZwDRmV',\n",
    " 'https://colab.research.google.com/drive/1L-Vx6dHcuOZDgtTOrk9k9AvOgYCum6dQ',\n",
    " 'https://colab.research.google.com/drive/1KeDmjC7XFRK5mX9OXHTjGMbVzViRUOEG',\n",
    " 'https://colab.research.google.com/drive/1oru8cEwM3EEZJqafKnSp0-FY-OG16C7_',\n",
    " 'https://colab.research.google.com/drive/12xQXXUXkApDNqvwXZKGJGPsCNbxnbvcR',\n",
    " 'https://colab.research.google.com/drive/1my4v1sJ954TLesO7CesaeQ-43Y9iizqB',\n",
    " 'https://colab.research.google.com/drive/19HmT3nrmY465GRcrME91k550Km9qpB9O',\n",
    " 'https://colab.research.google.com/drive/1KGK4DybzhOU3abJofcNOjT3nTh0ezzPy',\n",
    " 'https://colab.research.google.com/drive/1HuIQZClE7GAA-Qx-bgfYJeD-cEz7x69Z',\n",
    " 'https://colab.research.google.com/drive/15eYkWuWKMfAJF5yntXj7FB-0fVdjes6s',\n",
    " 'https://colab.research.google.com/drive/1OXvNXnZoRS1Px4NCsSULspE5hdlHmD81',\n",
    " 'https://colab.research.google.com/drive/1LEJxDd8mDp5qgM7fU4HN4OlUZc3LPOLg',\n",
    " 'https://colab.research.google.com/drive/1LLfohSnpQkrwWmnWn5Z870--iAEfNi8q',\n",
    " 'https://colab.research.google.com/drive/12Hy8zETbqkppKYsjGx4unURVJzT5yOre',\n",
    " 'https://colab.research.google.com/drive/1UsbsUJfmYjAYMT0gx6Vf31E-rHgdfN6v',\n",
    " 'https://colab.research.google.com/drive/1THjPNauR474s_9_KKCLNLJMCxdjazStr',\n",
    " 'https://colab.research.google.com/drive/1zTNyAMz3-MEL-v22KhoIPrh-fkr8QO4c',\n",
    " 'https://colab.research.google.com/drive/11j9ogx3XDIe_oHPNSrXLdo-ZRd9gTBj_',\n",
    " 'https://colab.research.google.com/drive/19cEqh_5Qs5XhqBpMWB7NUlK6xdJzz0Nl',\n",
    " 'https://colab.research.google.com/drive/1H1PuQtV_iMis0A2JYyGkhDK_yO86NWf9',\n",
    " 'https://colab.research.google.com/drive/1HK6UAoy6OVsvVRuuVR6g2Q2naeSfu2so',\n",
    " 'https://colab.research.google.com/drive/1osS7hdnErgBpMhVOo_pPYA23KheFjp3-',\n",
    " 'https://colab.research.google.com/drive/19rNQcINThr752-wOdnLfYMG_GZI-G1WA',\n",
    " 'https://colab.research.google.com/drive/1rFq481ihhQ9wM2w-VVrj6pfvIah9oZFL',\n",
    " 'https://colab.research.google.com/drive/1yrFXe0ydN1jIrxFUF8ABBHFq18kAb3wU',\n",
    " 'https://colab.research.google.com/drive/1Pp8oadtFk89wvdZFFtFyHr6OejVExjXE',\n",
    " 'https://colab.research.google.com/drive/1o7aEZrKPFuFCYhI4b2cNMjjiYveqXJ_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_batches = []\n",
    "for sheet_name in included_sheet_names:\n",
    "    print(sheet_name)\n",
    "    bdf = download_sheet_as_df(service_account_file, tracking_sheet_id, sheet_name)\n",
    "    progress_batches.append(bdf)\n",
    "\n",
    "df = pd.concat(progress_batches, ignore_index=True)\n",
    "\n",
    "\n",
    "df = df[df[\"task_link\"].isin(to_exclude_tasks)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
